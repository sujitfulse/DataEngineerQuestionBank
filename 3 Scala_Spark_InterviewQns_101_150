101	"reducebykey v/s groupby key ?

  Always try to use reducebykey instead of groupbykey. 
  The ReduceByKey and GroupByKey can perform almost similar functions, but GroupByKey contains large data. 
  Hence, try to use ReduceByKey to the most. Always try to lower the side of maps as much as possible. T
  ry not to waste more time in Partitioning.Try not to shuffle more. 
  Try to keep away from Skews as well as partitions too.

-----------------------------------------------------------------------------------------------------------------------------------------------------

102	"Why we need compression and what are the different compression format supported?

   In Big Data, when we used the compression, it saves the storage space and reduce the network overhead.
   One can specify the compression coded while writing the data to HDFS ( Hadoop format)
   One can also read the compressed data, for that also we can use compression codec.
   Following are the different compression format support in BigData:
   * gzip
   * lzo
   * bzip2
   * Zlib
   * Snappy"
-----------------------------------------------------------------------------------------------------------------------------------------------------
   
103	 Explain Spark streaming

   Spark Streaming
   A data stream defines as a data arriving continuously in the form of an unbounded sequence. 
   For further processing, Streaming separates continuously flowing input data into discrete units. 
   It is a low latency processing and analyzing of streaming data.
   We can do fault-tolerant,scalable stream processing of live data streams. 
   From many sources like Kafka, Apache Flume, Amazon Kinesis or TCP sockets, Data ingestion can be possible. 
   Also, by using complex algorithms, processing is possible. 
   That are expressed with high-level functions such as map, reduce, join and window. 
   By the end, processed data can be pushed out to filesystems, databases and live dashboards.

   Internally, By Spark streaming, Live input data streams are received and divided into batches. 
   Afterwards, these batches are then processed by the Spark engine to generate the final stream of results in batches.

   Discretized Stream or, in short, a Spark DStream is its basic abstraction. 
   That also represents a stream of data divided into small batches. DStreams are built on Spark RDDs, Spark’s core data abstraction. 
   Streaming can aslo integrate with any other Apache Spark components like Spark MLlib and Spark SQL."
   
-----------------------------------------------------------------------------------------------------------------------------------------------------  

104	"What is DStream in Apache Spark Streaming?


    A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous sequence of RDDs representing a continuous stream of data. 
    DStreams can either be created from live data (such as, data from HDFS, Kafka or Flume) or it can be generated by transformationexisting 
    DStreams using operations such as map, window and reduceByKeyAndWindow.

    Internally, there are few basic properties by which DStreams is characterized:

    1. DStream depends on the list of other DStreams.
    2. A time interval at which the DStream generates an RDD
    3. A function that is used to generate an RDD after each time interval"
    
-----------------------------------------------------------------------------------------------------------------------------------------------------  
    
 105	"Explain different transformations in DStream in Apache Spark Streaming


    1-map(func) — Return a new DStream by passing each element of the source DStream through a function func.

    2-flatMap(func) — Similar to map, but each input item can be mapped to 0 or more output items.

    3-filter(func) — Return a new DStream by selecting only the records of the source DStream on which func returns true.

    4-repartition(numPartitions) — Changes the level of parallelism in this DStream by creating more or fewer partitions.

    5-union(otherStream) — Return a new DStream that contains the union of the elements in the source DStream and
    otherDStream.

    6-count() — Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.

    7-reduce(func)— Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one).

    8-countByValue() — When called on a DStream of elements of type K, Return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.

    9-reduceByKey(func, [numTasks])— When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function.

    10-join(otherStream, [numTasks]) — When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.

    11-cogroup(otherStream, [numTasks]) — When called on DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.

    12-transform(func) — Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream.

    13-updateStateByKey(func) — Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key."


-----------------------------------------------------------------------------------------------------------------------------------------------------

106	"What is a DataSet? What are its advantages over DataFrame and RDD?

    Ans. In Apache Spark, Datasets are an extension of DataFrame API. It offers object-oriented programming interface. 
    Through Spark SQL, it takes advantage of Spark’s Catalyst optimizer by exposing e data fields to a query planner.

    In SparkSQL, Dataset is a data structure which is strongly typed and is a map to a relational schema.
    Also, represents structured queries with encoders. DataSet has been released in Spark 1.6.

    In serialization and deserialization (SerDe) framework, encoder turns out as a primary concept in Spark SQL. 
    Encoders handle all translation process between JVM objects and Spark’s internal binary format. 
    In Spark, we have built-in encoders those are very advanced. Even they generate bytecode to interact with off-heap data.

    On-demand access to individual attributes without having to de-serialize an entire object is provided by an encoder. 
    Spark SQL uses a SerDe framework, to make input-output time and space efficient. 
    Due to encoder knows the schema of record, it became possible to achieve serialization as well as deserialization.

    Spark Dataset is structured and lazy query expression(lazy Evolution) that triggers the action. 
    Internally dataset represents a logical plan. The logical plan tells the computational query that we need to produce the data.
    the logical plan is a base catalyst query plan for the logical operator to form a logical query plan.
    When we analyze this and resolve we can form a physical query plan.

    As Dataset introduced after RDD and DataFrame, it clubs the features of both. It offers the following similar features:

    1. The convenience of RDD.
    2. Performance optimization of DataFrame.
    3. Static type-safety of Scala.

    Hence, we have observed that Datasets provides a more functional programming interface to work with structured data."
 
 -----------------------------------------------------------------------------------------------------------------------------------------------------  

107	"On what all basis can you differentiate RDD, DataFrame, and DataSet?

   Ans. 
   DataFrame: 
   A Data Frame is used for storing data into tables. It is equivalent to a table in a relational database but with richer optimization.
   Spark DataFrame is a data abstraction and domain-specific language (DSL) applicable on a structure and semi-structured data. 
   It is distributed collection of data in the form of named column and row. 
   It has a matrix-like structure whose column may be different types (numeric, logical, factor, or character ).
   We can say data frame has the two-dimensional array like structure where each column contains the value of one variable and
   row contains one set of values for each column and combines feature of list and matrices.

   RDD: 
   RDD is the representation of a set of records, immutable collection of objects with distributed computing. 
   RDD is a large collection of data or RDD is an array of reference of partitioned objects. 
   Each and every dataset in RDD is logically partitioned across many servers so that they can compute on different nodes of the cluster. 
   RDDs are fault tolerant i.e. self-recovered/recomputed in the case of failure. 
   The dataset can load externally by the users which can be in the form of JSON file, CSV file, text file or database via JDBC with no specific 
   data structure.

   DataSet: 
   Datasets are an extension of DataFrame API. 
   It offers object-oriented programming interface. 
   Through Spark SQL, it takes advantage of Spark’s Catalyst optimizer by exposing e data fields to a query planner."

-----------------------------------------------------------------------------------------------------------------------------------------------------  

108	Explain Catalyst framework.

   The Catalyst is a framework which represents and manipulate a DataFrame graph. 
   Data flow graph is a tree of relational operator and expressions. The three main features of catalyst are:

   It has a TreeNode library for transforming tree. They are expressed as Scala case classes.
   A logical plan representation for relational operator.
   Expression library.
   The TreeNode builds a query optimizer. It contains a number of the query optimizer.
   Catalyst Optimizer supports both rule-based and cost-based optimization. 
   In rule-based optimization the optimizer use set of rule to determine how to execute the query.
   While the cost based optimization finds the most suitable way to carry out SQL statement. 
   In cost-based optimization, many plans are generates using rules. And after this, it computes their cost. 
   Catalyst optimizer makes use of standard features of Scala programming like pattern matching."
-----------------------------------------------------------------------------------------------------------------------------------------------------  

109	Does Apache Spark provide checkpointing?

   Yes, Apache Spark provides checkpointing.

   Apache supports two types of checkpointing:

   Reliable Checkpointing: It refers to that checkpointing in which the actual RDD is saved in the reliable distributed file system, e.g. HDFS. 
   To set the checkpoint directory call: SparkContext.setCheckpointDir(directory: String). 
   When running on the cluster the directory must be an HDFS path since the driver tries to recover the checkpointed RDD from a local file. 
   While the checkpoint files are actually on the executor’s machines.

   Local Checkpointing: In this checkpointing, in Spark Streaming or GraphX we truncate the RDD lineage graph in Spark. 
   In this, the RDD is persisted to local storage in the executor."

-----------------------------------------------------------------------------------------------------------------------------------------------------  
110	"What is Hive on Spark?
  Hive contains significant support for Apache Spark, wherein Hive execution is configured to Spark:

  hive> set spark.home=/location/to/sparkHome;
  hive> set hive.execution.engine=spark;"
-----------------------------------------------------------------------------------------------------------------------------------------------------  
111	"Is it possible to use Apache Spark for accessing and analyzing data stored in Cassandra databases?
  Answer: Yes, it is possible to use Apache Spark for accessing as well as analyzing data stored in Cassandra databases using the 
  Spark Cassandra Connector. It needs to be added to the Spark project during which a Spark executor talks to a local Cassandra node 
  and will query only local data.
  Connecting Cassandra with Apache Spark allows making queries faster by means of reducing the usage of the network for sending data between 
  Spark executors and Cassandra nodes."
-----------------------------------------------------------------------------------------------------------------------------------------------------  
112 What is the difference between reducing () and take() function?
  Answer: 
  Reduce() function is an action that is applied repeatedly until the one value is left in the last.
  take() function is an action that takes into consideration all the values from an RDD to the local node.
-----------------------------------------------------------------------------------------------------------------------------------------------------  
113 Can we trigger automated clean-ups in Spark?
   Answer: Yes, we can trigger automated clean-ups in Spark to handle the accumulated metadata. 
   It can be done by setting the parameters, namely, “spark.cleaner.ttl.” 
   
   Question: What is another method than “Spark.cleaner.ttl” to trigger automated clean-ups in Spark?
   Answer: Another method than “Spark.clener.ttl” to trigger automated clean-ups in Spark is by dividing the long-running jobs into 
   different batches and writing the intermediary results on the disk."
-----------------------------------------------------------------------------------------------------------------------------------------------------  
114	What is the role of Akka in Spark?
   Answer: Akka in Spark helps in the scheduling process. It helps the workers and masters to send and receive messages for 
   workers for tasks and master requests for registering."
-----------------------------------------------------------------------------------------------------------------------------------------------------  
