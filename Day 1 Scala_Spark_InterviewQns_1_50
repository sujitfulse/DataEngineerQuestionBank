1	What is difference between order by and sort by ? 
  They are NOT the SAME.
  The SORT BY clause is used to return the result rows sorted within each partition in the user specified order. 
  When there is more than one partition SORT BY may return result that is partially ordered.
  Reference :https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sortby.html
  The ORDER BY clause is used to return the result rows in a sorted manner in the user specified order. 
  Unlike the SORT BY clause, this clause guarantees a total order in the output.
  Reference : https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-orderby.html
----------------------------------------------------------------------------------------------------------------------------
2	What are the important requirements For Spark Submit
    spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  <application-arguments>
----------------------------------------------------------------------------------------------------------------------------
3	What is mean by Broadcast Variable in spark ? 
Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. 
They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. 
Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.

import org.apache.spark.sql.SparkSession
object BroadcastExample extends App{

  val spark = SparkSession.builder()
    .appName("SparkByExamples.com")
    .master("local")
    .getOrCreate()

  val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
  val countries = Map(("USA","United States of America"),("IN","India"))

  val broadcastStates = spark.sparkContext.broadcast(states)
  val broadcastCountries = spark.sparkContext.broadcast(countries)

  val data = Seq(("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  )

  val columns = Seq("firstname","lastname","country","state")
  import spark.sqlContext.implicits._
  val df = data.toDF(columns:_*)

  val df2 = df.map(row=>{
    val country = row.getString(2)
    val state = row.getString(3)

    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (row.getString(0),row.getString(1),fullCountry,fullState)
  }).toDF(columns:_*)

  df2.show(false)
}

----------------------------------------------------------------------------------------------------------------------------
 
4	How to utilize hive buckets in spark ?

Bucketing is a partitioning technique that can improve performance in certain data transformations by avoiding data shuffling and sorting. 
The general idea of bucketing is to partition, and optionally sort, the data based on a subset of columns while it is written out (a one-time cost), 
Bucketing can enable faster joins (i.e. single stage sort merge join), 
the ability to short circuit in FILTER operation if the file is pre-sorted over the column in a filter predicate, and it supports quick data sampling.

Hive bucketed tables are supported from Spark 2.3 onwards. 
Spark normally disallow users from writing outputs to Hive Bucketed tables.
Setting hive.enforce.bucketing=false and hive.enforce.sorting=false will allow you to save to Hive Bucketed tables.

 Spark still won't produce bucketed data as per Hive's bucketing guarantees, 
 but will allow writes IFF user wishes to do so without caring about bucketing guarantees. 
 Ability to create bucketed tables will enable adding test cases to Spark while pieces are being added to Spark have it support hive bucketing 

//enable Hive support when creating/configuring the spark session
val spark = SparkSession.builder().enableHiveSupport().getOrCreate()

//register DF as view that can be used with SparkSQL
val testDF = Seq((1, "a"),(2, "b"),(3, "c")).toDF("number", "letter")
testDF.createOrReplaceTempView("testDF")

//create Hive table, can also be done manually, e.g. via Hive CLI
val createTableSQL = "CREATE TABLE testTable (number int, letter string) CLUSTERED BY number INTO 1 BUCKETS STORED AS PARQUET"
spark.sql(createTableSQL)

//load data from DF into Hive, output parquet files will be bucketed and readable by Hive
spark.sql("INSERT INTO testTable SELECT * FROM testDF")

https://community.cloudera.com/t5/Support-Questions/Hive-bucketed-table-from-Spark-2-3/td-p/221572
https://medium.com/@deepa.account/comparison-between-spark-and-hive-bucketing-827ec41fe58d
----------------------------------------------------------------------------------------------------------------------------

5	Write a Spark Scala Code to Create Tempview from a Text file
val textFile = sc.textFile("hdfs://localhost:9000/user/input.txt")
textFile.createOrReplaceTempView("testDF")
----------------------------------------------------------------------------------------------------------------------------

6	We have Two Input files: .csv and .parquet with policy and insured details. You have to join based on some command column and run some business logic to hive table Using data frame.
 write sparksql this was supposed to be done"

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

object checkDFSchema extends App {
  val cc = new SparkConf;
  val sc = new SparkContext(cc)
  val sparkSession = SparkSession.builder().enableHiveSupport().getOrCreate()
  //First option for creating hive table through dataframe 
  val DF = sparkSession.sql("select * from salary")
  DF.createOrReplaceTempView("tempTable")
  sparkSession.sql("Create table yourtable as select * form tempTable")
  //Second option for creating hive table from schema
  val oldDFF = sparkSession.sql("select * from salary")
  //Generate the schema out of dataframe  
  val schema = oldDFF.schema
  //Generate RDD of you data 
  val rowRDD = sc.parallelize(Seq(Row(100, "a", 123)))
  //Creating new DF from data and schema 
  val newDFwithSchema = sparkSession.createDataFrame(rowRDD, schema)
  newDFwithSchema.createOrReplaceTempView("tempTable")
  sparkSession.sql("create table FinalTable AS select * from tempTable")
}
https://stackoverflow.com/questions/42261701/how-to-create-hive-table-from-spark-data-frame-using-its-schema
----------------------------------------------------------------------------------------------------------------------------

7	How the folder structure is constructed in ur project in Eclipse IDE
src/main/java ==> java code 
src/test/java ==> test cases
Maven Depdencies ==> all jars
pom.xml
----------------------------------------------------------------------------------------------------------------------------

8	How do u access the global variable in ur spark code ? 
 you can use broadcast mechanism to access global variable. but it is immutable.
 Spark is built on the principle of immutability, in fact any distributed framework works by leveraging the concepts of immutability. 
 so you can not modify global variable.
 https://stackoverflow.com/questions/36401537/how-to-define-a-global-read-write-variables-in-spark
----------------------------------------------------------------------------------------------------------------------------
9	What are the starting lines you write in spark code.
// declair spark configuration object 
// declair spark context 
// build spark session 
e.g. 
  val cc = new SparkConf;
  val sc = new SparkContext(cc)
  val sparkSession = SparkSession.builder().getOrCreate()
  val DF = sparkSession.sql("select * from salary") 
----------------------------------------------------------------------------------------------------------------------------  
10	Explode function in Spark
convert collection (array/ maps) columns to rows in order to process in Spark effectively.
Spark explode functions -> 
A) explode ==> This will ignore elements that have null or empty datatypes. 
B) explode_outer ==> This will not ignore elements that have null or empty datatypes. 
C) posexplode  ==> This will ignore elements that have null or empty datatypes. it will add one more column to show position i.e. index number in array/map
D) posexplode_outer==> This will not ignore elements that have null or empty datatypes. it will add one more column to show position i.e. index number in array/map
https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/
----------------------------------------------------------------------------------------------------------------------------  

11	What is case class in spark
Case classes uses reflection to generate the schema of an RDD that contains specific types of objects. 
The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. 
The case class defines the schema of the table. 
The names of the arguments to the case class are read using reflection and they become the names of the columns.
Case classes can also be nested or contain complex types such as Sequences or Arrays. 
This RDD can be implicitly be converted to a DataFrame and then registered as a table. Tables can be used in subsequent SQL statements.
example : 
data 
1201, satish, 25
1202, krishna, 28
1203, amith, 39
1204, javed, 23
1205, prudvi, 23

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
scala> import sqlContext.implicts._
scala> case class Employee(id: Int, name: String, age: Int)
scala> val empl=sc.textFile("employee.txt")
.map(_.split(","))
.map(e⇒ employee(e(0).trim.toInt,e(1), e(2).trim.toInt))
.toDF()
scala> empl.registerTempTable("employee")
scala> val allrecords = sqlContext.sql("SELeCT * FROM employee")
https://www.tutorialspoint.com/spark_sql/inferring_schema_using-reflection.htm#:~:text=The%20Scala%20interface%20for%20Spark,the%20names%20of%20the%20columns.
----------------------------------------------------------------------------------------------------------------------------

12	How to process nested json or nested array in spark
    Step 1: Load JSON data into Spark Dataframe using API
    Step 2: Explode Array datasets in Spark Dataframe.
    Step 3: Fetch each order using GetItem on Explored columns.
    Step 4: Repeat step 1 to 4 , untill we flatten all the object to column lelel.
 
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

 val ordersDf = spark.read.format("json")
                         .option("inferSchema", "true")
                         .option("multiLine", "true")
                         .load("/FileStore/tables/orders_sample_datasets.json")
 import org.apache.spark.sql.functions._
 val parseOrdersDf = ordersDf.withColumn("orders", explode($"datasets"))      
// Step 3: Fetch Each Order using getItem on explode column
 val parseOrdersDf = parseOrdersDf.withColumn("customerId", $"orders".getItem("customerId"))
                             .withColumn("orderId", $"orders".getItem("orderId"))
                             .withColumn("orderDate", $"orders".getItem("orderDate"))
                             .withColumn("orderDetails", $"orders".getItem("orderDetails"))
                             .withColumn("shipmentDetails", $"orders".getItem("shipmentDetails"))                  
                         
   https://bigdataprogrammers.com/read-nested-json-in-spark-dataframe/
----------------------------------------------------------------------------------------------------------------------------
13	How to select few columns from an rdd and not converting it to a df
   - use case class or struct to define DF column names.
   - use rdd.map() and then choose selected column only.
----------------------------------------------------------------------------------------------------------------------------
14	If the data type is not correct, then How to enforce the data type on a column after the df is created

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.DataType
import org.apache.spark.sql.types.{StringType, StructType,IntegerType,StructField}

val simpleData = Seq(Row("James ","","Smith","36636","M",3000),
    Row("Michael ","Rose","","40288","M",4000),
    Row("Robert ","","Williams","42114","M",4000),
    Row("Maria ","Anne","Jones","39192","F",4000),
    Row("Jen","Mary","Brown","","F",-1)
  )

val simpleSchema = StructType(Array(
    StructField("firstname",StringType,true),
    StructField("middlename",StringType,true),
    StructField("lastname",StringType,true),
    StructField("id", StringType, true),
    StructField("gender", StringType, true),
    StructField("salary", IntegerType, true)
  ))

 val df = spark.createDataFrame(spark.sparkContext.parallelize(simpleData),simpleSchema)
 df.printSchema()
 df.show()
 // NOTE : if data type is not matched then it will fail 
  https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/
----------------------------------------------------------------------------------------------------------------------------
15	How to read parquet file in spark

val parqDF = spark.read.parquet("/tmp/output/people.parquet")
df.write.parquet("/tmp/output/people.parquet")
----------------------------------------------------------------------------------------------------------------------------
16	Fold vs Left fold vs right fold difference in Scala

foldLeft starts on the left side—the first item—and iterates to the right; FoldLeft is a iterative solution.
foldRight starts on the right side—the last item—and iterates to the left. FoldRight: is a recursive solution 
fold goes in no particular order.

val numList = List(1, 2, 3, 4, 5)

val r1 = numList.par.fold(0)((acc, value) => {
  println("adding accumulator=" + acc + ", value=" + value + " => " + (acc + value))
  acc + value
})
println("fold(): " + r1)
println("#######################")

/*
 * You can see from the output that,
 * fold process the elements of parallel collection in parallel
 * So it is parallel not linear operation.
 * 
 * adding accumulator=0, value=4 => 4
 * adding accumulator=0, value=3 => 3
 * adding accumulator=0, value=1 => 1
 * adding accumulator=0, value=5 => 5
 * adding accumulator=4, value=5 => 9
 * adding accumulator=0, value=2 => 2
 * adding accumulator=3, value=9 => 12
 * adding accumulator=1, value=2 => 3
 * adding accumulator=3, value=12 => 15
 * fold(): 15
 */

val r2 = numList.par.foldLeft(0)((acc, value) => {
  println("adding accumulator=" + acc + ", value=" + value + " => " + (acc + value))
  acc + value
})
println("foldLeft(): " + r2)
println("#######################")
/*
 * You can see that foldLeft
 * picks elements from left to right.
 * It means foldLeft does sequence operation
 * 
 * adding accumulator=0, value=1 => 1
 * adding accumulator=1, value=2 => 3
 * adding accumulator=3, value=3 => 6
 * adding accumulator=6, value=4 => 10
 * adding accumulator=10, value=5 => 15
 * foldLeft(): 15
 * #######################
 */

// --> Note in foldRight second arguments is accumulated one.
val r3 = numList.par.foldRight(0)((value, acc) => {
 println("adding value=" + value + ", acc=" + acc + " => " + (value + acc))
  acc + value
})
println("foldRight(): " + r3)
println("#######################")

/*
 * You can see that foldRight
 * picks elements from right to left.
 * It means foldRight does sequence operation.
 * 
 * adding value=5, acc=0 => 5
 * adding value=4, acc=5 => 9
 * adding value=3, acc=9 => 12
 * adding value=2, acc=12 => 14
 * adding value=1, acc=14 => 15
 * foldRight(): 15
 * #######################
 */
----------------------------------------------------------------------------------------------------------------------------

17	Map vs flat map difference in scala.
map returns only one element.
flatMap returns a list of elements (0 or more) as an iterator.

val data=sc.textFile("sparkdata.txt")  
val splitdata = data.flatMap(line => line.split(" "));  
var splitdataPair = splitdata.map((_, 1))
val reducedata = splitdataPair.reduceByKey(_+_); 
reducedata.saveAsTextFile("my_spark_shell_wc_output")
----------------------------------------------------------------------------------------------------------------------------
18 19 moved 
----------------------------------------------------------------------------------------------------------------------------
20 moved 
----------------------------------------------------------------------------------------------------------------------------

21	what are different spark distributions ?
-- HDP 
-- CDP
----------------------------------------------------------------------------------------------------------------------------
22 	What are the differences between Spark 1.x and Spark 2.x and Spark 3.x ? 

*Spark 2.x :-
a) SparkSession - 
-A new entry point for Spark application execution is introduced. There are several reasons for introducing it. 
-Some of them are: we need to create separately sql context, hive context if we have only SparkContext.
b) Faster analysis - 
-Spark 1.x uses compilers which uses of several function calls and CPU cycles, because of which so much unnecessary work spent on CPU cycles.
-Spark 2.x uses performance enhanced Tungsten engine. 
-This new version of Tungsten project takes the query plan and collapses it into a single function by avoiding unnecessary function calls. 
-It uses CPU registers to store intermediate instead using CPU memory as that of Spark 1.x. This will improve the performance by 10 time at least..
c) Added SQL features - 
-SQL in Spark 2.x added with more functionalities along with support for SQL2003. 
-Spark SQL in 2.x version can run all of the 99 TPC-DS queries. 
-Hive support will have all the features when it create with Hive support. 
-Native SQL parser which supports both ANSI - SQL and Hive QL. Native DDL commands are implemented.
d) MLlib improvements - 
- RDD based API is going into maintenance mode and DataFrame based has become the primary API now. 
- Many machine learning algorithms like Gaussian Mixture Model, MaxAbsScaler, Bisecting K-Means clustering feature transformer are added to DataFrame based API.
- many ML algorithms added to PySpark and SparkR also.
e) New Streaming module - 
- In Spark 2.x a new high level streaming API built on Spark SQL and Catalyst Optimizer has been added. 
- It is added as a separate module in Spark release with the name Structured Streaming.
- Using this users can connect to any streaming source and sink leveraging the optimization techniques which come with DataFrame API. 
- for DataFrame API support for connection with Kafka has been added but it is experimental stage.
f) Unified Dataset and DataFrame APIs 
- DataFrame  is a high level API for structured data introduced since 1.3 version of Spark. 
- This has lot performance optimizations compared to RDD. 
- Later in 1.6 version of Spark new API for structured data is introduced, DataSet. 
- Dataset provide type safety compared DataFrames. 
- In Spark 2.x onwards these two are unified to form new single API. 
- Now Dataframe is just an alias for Dataset of Row. Datasets will have all the typed and untyped methods. 

*Spark 3.0
A. Adaptive Query Execution (AQE) enhancements
- Unlike more traditional technologies, runtime adaptivity in Spark is crucial as it enables the optimization of execution plans based on the input data. 
The reason why this is so important in Spark is due to the fact that the data itself affects the efficiency of the application. 
Good example that demonstrates the importance of the dynamic adaptation of execution plans is broadcasting and data skewness. 
- Spark 3.0 introduced two major improvements over Adaptive Query Execution :
 -- AQE now combines small partitions together so that users don’t need to worry too much about shuffle partitions since this will now be dynamically adjusted in runtime.
 -- AQE automatically breaks down partitions into smaller ones once data skewness is detected

B. Improvements on pandas UDF API
The new interface allows pandas UDFs to infer the type from the given Python type hints in the Python function definition.
import pandas as pd
from pyspark.sql.functions import pandas_udf

@pandas_udf('long')
def pandas_subtract_unit(s: pd.Series) -> pd.Series:
    return s - 1
    
C. New User Interface for Structured Streaming    
- Web UI in Apache Spark 3.0 comes with an extra tab which is dedicated to Structured Streaming and simplifies monitoring over streaming jobs.
- existing metrics :
Input Rate
Process Rate
Input Rows
Batch Duration
Operation Duration
- newley added 
scheduling delay and processing time for each micro-batch

----------------------------------------------------------------------------------------------------------------------------

23	Difference between SparkContext and SparkSession:
- In Spark version 1.x we have the SparkContext as entry point for out Spark applications, for which we need to create SparkConf object. 
All our configurations will be set to SparkConf object and using which SparkContext object will be created as shown below:
import org.apache.spark.{SparkConf, SparkContext}
val conf = new SparkConf().setAppName(“MyAppliction”).setMaster(“local”) 
val sparkContext = new SparkContext(conf)

- Whereas in Spark version 2.x we have SparkSession as entry point. Which is completely different from SparkContext. 
Using this SparkSession we create DataFrames and Datasets and accomplish the task. 
org.apache.spark.sql.SparkSession
val spark = SparkSession
            .builder
            .master("local")
            .appName(“WorldBankIndex”)
            .getOrCreate()
val context = spark.sparkContext

- Spark Session also includes all the APIs available in different contexts –

Spark Context,
SQL Context,
Streaming Context,
Hive Context.

- If you observe two different SparkSessions created in an application will have the same SparkContext object. 
Another reason for having only one SparkContext object and multiple SparkSesssions is multiple users to process the data. 
If we are using Spark version 1.6 and we have multiple users to operate on the datasets and 
need the datasets to be isolated for each user then we have to create multiple SparkContext objects, 
which is not at all acceptable in performance perception. 
Whereas for the same situation is handled in Spark version 2.x by creating multiple SparkSessions and single SparkContext. 
----------------------------------------------------------------------------------------------------------------------------

24	How to convert rdd to data frame
	A) using toDF
	 rdd.toDF()
	 rdd.toDF("language","users_count")
	B) Using createDataFrame()
	val columns = Seq("language","users_count")
	val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)
	- using StructType
	val schema = StructType(columns.map(fieldName => StructField(fieldName, StringType, nullable = true)))
        val rowRDD = rdd.map(attributes => Row(attributes._1, attributes._2))
        val dfFromRDD3 = spark.createDataFrame(rowRDD,schema)
	C) RDD to DataSet :
       val ds = spark.createDataset(rdd)
----------------------------------------------------------------------------------------------------------------------------
25	what is struct type ? is it possible to define array field in struct

 val arrayStructData = Seq(
      Row("James",List(Row("Java","XX",120),Row("Scala","XA",300))),
      Row("Michael",List(Row("Java","XY",200),Row("Scala","XB",500))),
      Row("Robert",List(Row("Java","XZ",400),Row("Scala","XC",250))),
      Row("Washington",null)
    )

    val arrayStructSchema = new StructType()
      .add("name",StringType)
      .add("booksIntersted",ArrayType(new StructType().add("name",StringType).add("author",StringType).add("pages",IntegerType)))

    val df = spark.createDataFrame(spark.sparkContext.parallelize(arrayStructData),arrayStructSchema)
  
    df.printSchema()
    df.show()

----------------------------------------------------------------------------------------------------------------------------

26	is it possible to convert a df into rdd
	YES. using  df.rdd
	ouput is rdd
----------------------------------------------------------------------------------------------------------------------------

27	Hive and spark optimizations techniques
	
	- Spark Code level optimization
	a) Use DataFrame/Dataset over RDD
	b) Use coalesce() over repartition()
	c) Use mapPartitions() over map()
	d) Use Serialized data format’s - avro, parquet
	e) Avoid UDF’s (User Defined Functions)
	f) Caching data in memory
	g) Reduce expensive Shuffle operations
	h) use adanced variables like - KRYO
	i) Use reduceByKey instead of groupByKey.
	j) tune default partition number       
----------------------------------------------------------------------------------------------------------------------------

28	how to replace null values with some other value or discard the rows with null values in spark
	// replace null with zero 
	df.na.fill(0)
	// replace null with empty string 
	df.na.fill("")
	// replace null with array datatype
	df.na.fill("",Array("type"))
	// if any column value is null then drop using 
	df.na.drop()
	// if all columns are null then only drop
	df.na.drop("all")
	// if selected column are null then drop using 
	df.na.drop("all", Seq("col1", "col2", "col3"))
----------------------------------------------------------------------------------------------------------------------------
29	how to enforce datatype checks on column level in scala
	val columns = df.schema.fields.filter(x => x.dataType == IntegerType || x.dataType == DoubleType)
----------------------------------------------------------------------------------------------------------------------------
30	Spark architecture, yarn architecture

----------------------------------------------------------------------------------------------------------------------------
31 moved
----------------------------------------------------------------------------------------------------------------------------
32 moved
----------------------------------------------------------------------------------------------------------------------------
  
33  Higher order function? and its advantages?

   A function is called Higher Order Function if it contains other functions as a parameter or returns a function as an output 
   i.e, the functions that operate with another functions are known as Higher order Functions. 
   
   a) it is beneficial in producing function composition where, functions might be formed from another functions. 
   The function composition is the method of composing where a function shows the utilization of two composed functions.
   b) It is also constructive in creating lambda functions or anonymous functions. 
   The anonymous functions are the functions which does not has name, though perform like a function.
   c) It is even utilized in minimizing redundant lines of code from a program.
----------------------------------------------------------------------------------------------------------------------------

34	Why you used Scala for Spark ?
	- Usually, Python is suitable for smaller projects, and Scala works best for large-scale ones. 
	- If faster performance is a requirement, Scala is a good bet. Spark is native in Scala, hence making writing Spark jobs in Scala the native way.
        - Scala is static-typed, while Python is a dynamically typed language. Due to its nature, the former is more suitable for projects dealing with high volumes of data.
----------------------------------------------------------------------------------------------------------------------------
35  How do you do packaging (managing packages) ? How do you add dependencies for your project/application? Why do you need dependencies?
        - in pom.xml
	- using depedency tag
	- so that we can use third party jars 
----------------------------------------------------------------------------------------------------------------------------
36	pom.xml <- will things work if we rename pom.xml
	- NO 
----------------------------------------------------------------------------------------------------------------------------
37	what are minimum mandatory imports that are required for scala application.
	- 
	import org.apache.spark.SparkContext
	import org.apache.spark.SparkContext._
	import org.apache.spark.SparkConf
----------------------------------------------------------------------------------------------------------------------------
38 moved 
