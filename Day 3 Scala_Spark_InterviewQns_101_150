
-----------------------------------------------------------------------------------------------------------------------------------------------------  
101	 diff map partition v/s foreach partition ?
mapPartitions and foreachPartitions are transformations/operations that apply to each partition of the Dataframe as opposed to each element. 
-----------------------------------------------------------------------------------------------------------------------------------------------------  
102 What is the difference between reducing () and take() function?
  Answer: 
  Reduce() function is an action that is applied repeatedly until the one value is left in the last.
  take() function is an action that takes into consideration all the values from an RDD to the local node.
-----------------------------------------------------------------------------------------------------------------------------------------------------  

103	"How to check no. Of partitions in dataframe 
    In my experience df.rdd.getNumPartitions is very fast, I never encountered taking this more than a second or so.
    Alternatively, you could also try
    val numPartitions: Long = df.select(org.apache.spark.sql.functions.spark_partition_id()).distinct().count()"
-----------------------------------------------------------------------------------------------------------------------------------------------------------
104	handling schema changes in spark, schema chaining in spark
https://medium.com/data-arena/merging-different-schemas-in-apache-spark-2a9caca2c5ce"
-----------------------------------------------------------------------------------------------------------------------------------------------------------
105	differnece between map and mappartion
map() – Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.
mapPartitions() – This is exactly the same as map(); the difference being, Spark mapPartitions() provides a facility to do heavy initializations 
(for example Database connection) once for each partition instead of doing it on every DataFrame row. 
This helps the performance of the job when you dealing with heavy-weighted initialization on larger datasets.
-----------------------------------------------------------------------------------------------------------------------------------------------------------
106	Why serializer is better .... What is serializer and deserializer
 serialization framework helps you convert objects into a stream of bytes and vice versa in new computing environment. 
 This is very helpful when you try to save objects to disk or send them through networks. Those situations happen in Spark when things are shuffled around.
----------------------------------------------------------------------------------------------------------------------------------------------------------- 
107	How to union two tables with different no. Of columns. 
==> no you cant. need to have simillar column in both dfs.
----------------------------------------------------------------------------------------------------------------------------------------------------------- 
108	what is the predicate pushdown ? what is the limitation ?
A predicate push down filters the data in the database query, reducing the number of entries retrieved from the database and improving query performance.
----------------------------------------------------------------------------------------------------------------------------------------------------------- 
109	 how to optimize spark code ?

      Data Serialization. ...
      Broadcasting. ...
      Avoid UDF and UDAF. ...
      Data locality. ...
      Dynamic allocation. ...
      Garbage collection. ...
      Executor Tuning. ...
      Parallelism.

      Garbage collection
      Garbage collection can be a bottleneck in spark applications. I
      t is advisable to try the G1GC garbage collector, which can improve the performance if garbage collection is the bottleneck. 
      The parameter -XX:+UseG1GC is used to specify G1GC as the default garbage collector. 
      And increase the G1GC region heap size if the executer heap size is large this can done by -XX:G1HeapRegionSize.
      # -XX:+UseG1GC
      # -XX:G1HeapRegionSize
----------------------------------------------------------------------------------------------------------------------------------------------------------- 
110 	"write a program where we can filter all null values and concat an '_' with all non null values

  val newDf =   df.withColumn("NEW_COLUMN",concat(when(col(""COL1"").isNotNull, col(""COL1"")),'-'));
------------------------------------------------------------------------------------------------------------------------------------------------------------  
 
111	What are the roles and responsibilities of worker nodes in the Apache Spark cluster? Is Worker Node in Spark is same as Slave Node?

      Apache Spark follows a master/slave architecture, with one master or driver process and more than one slave or worker processes
      
      1. The master is the driver that runs the main() program where the spark context is created. 
         It then interacts with the cluster manager to schedule the job execution and perform the tasks.

      2. The worker consists of processes that can run in parallel to perform the tasks scheduled by the driver program. 
         These processes are called executors.

      Whenever a client runs the application code, the driver programs instantiates Spark Context, converts the transformations and 
      actions into logical DAG of execution. 
      This logical DAG is then converted into a physical execution plan, which is then broken down into smaller physical execution units. 
      The driver then interacts with the cluster manager to negotiate the resources required to perform the tasks of the application code. 
      The cluster manager then interacts with each of the worker nodes to understand the number of executors running in each of them.

      The role of worker nodes/executors:

      1. Perform the data processing for the application code
      2. Read from and write the data to the external sources
      3. Store the computation results in memory, or disk.
     
      Before the execution of tasks, the executors are registered with the driver program through the cluster manager, 
      so that the driver knows how many numbers of executors are running to perform the scheduled tasks. 
      The executors then start executing the tasks scheduled by the worker nodes through the cluster manager.

      Whenever any of the worker nodes fail, the tasks that are required to be performed will be automatically allocated to any other worker nodes"
------------------------------------------------------------------------------------------------------------------------------------------------------- 

112	How to chain Dataframe transformation in Spark Transform method ?
    
  implicit classes or the Dataset#transform method can be used to chain DataFrame transformations in Spark.
  Though Dataset#transform method is preferred compared to implicit classes.

      Eg:

      val df = Seq("funny","person").toDF("something")

      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }

      def withFarewell(df: DataFrame): DataFrame = {
        df.withColumn(""farewell"", lit(""goodbye""))
      }


      val weirdDf = df
        .transform(withGreeting)
        .transform(withFarewell)
    
       weirdDf.show()
      +---------+-----------+--------+
      |something|   greeting|farewell|
      +---------+-----------+--------+
      |    funny|hello world| goodbye|
      |   person|hello world| goodbye|

      Transform Method with Arguments
-----------------------------
      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }
      def withCat(name: String)(df: DataFrame): DataFrame = {
        df.withColumn(""cats"", lit(s""$name meow""))
      }

      val niceDf = df
        .transform(withGreeting)
        .transform(withCat(""puffy""))
---------------------------

      Monkey Patching with Implicit Classes :
      Implicit classes can be used to add methods to existing classes. 
      The following code adds the same withGreeting() and withFarewell() methods to the DataFrame class itself.

      object BadImplicit {
        implicit class DataFrameTransforms(df: DataFrame) {
          def withGreeting(): DataFrame = {
            df.withColumn(""greeting"", lit(""hello world""))
          }
          def withFarewell(): DataFrame = {
            df.withColumn(""farewell"", lit(""goodbye""))
          }
        }
      }
      The withGreeting() and withFarewell() methods can be chained and executed as follows.

      import BadImplicit._
      val df = Seq("funny","person").toDF(""something"")
      val hiDf = df.withGreeting().withFarewell()"
------------------------------------------------------------------------------------------------------------------------------------------------------- 
113	difference between map and foreach ? when to use a map and when to use foreach ?

      map :
      the map function in the Seq trait returns a value.
      It is designed to apply a function to every element of a collection extending the Seq trait and return a new collection.
      def map[B](f: (A) ⇒ B): Seq[B]

      foreach, in the same trait, has the following signature:
      def foreach(f: (A) ⇒ Unit): Unit

      val l: List[String] = List("a", "b", "c")
      l.map(e => e.toUpperCase(java.util.Locale.ROOT))
      l.foreach(e => println(e))
------------------------------------------------------------------------------------------------------------------------------------------------------- 

114	what is case class and when we will use a case class ?

    Scala case classes are just regular classes which are immutable by default and decomposable through pattern matching.
    It uses equal method to compare instance structurally. It does not use new keyword to instantiate object.
    All the parameters listed in the case class are public and immutable by default.
    
    case class className(parameters)  
    
    case class CaseClass(a:Int, b:Int)    
    object MainObject{  
        def main(args:Array[String]){  
            var c =  CaseClass(10,10)       // Creating object of case class  
            println("a = "+c.a)               // Accessing elements of case class  
            println("b = "+c.b)  
        }  
    }  

A case class which has no arguments is declared as case object instead of case class. case object is serializeable by default.

      trait SuperTrait  
      case class CaseClass1(a:Int,b:Int) extends SuperTrait  
      case class CaseClass2(a:Int) extends SuperTrait         // Case class  
      case object CaseObject extends SuperTrait               // Case object  
      object MainObject{  
          def main(args:Array[String]){  
              callCase(CaseClass1(10,10))  
              callCase(CaseClass2(10))  
              callCase(CaseObject)  
          }  
          def callCase(f:SuperTrait) = f match{  
              case CaseClass1(f,g)=>println("a = "+f+" b ="+g)  
              case CaseClass2(f)=>println("a = "+f)  
              case CaseObject=>println("No Argument")  
          }  
      }  
------------------------------------------------------------------------------------------------------------------------------------------------------- 

115	spark scala word count programm

RDD : 
 val text = sc.textFile("mytextfile.txt") 
 val counts = text.flatMap(line => line.split(" ") ).map(word => (word,1)).reduceByKey(_+_) counts.collect 

DataFrame :
val df = sqlContext.read.text("README.md")
val wordsDF = df.select(split(df("value")," ").alias("words"))
val wordDF = wordsDF.select(explode(wordsDF("words")).alias("word"))
val wordCountDF = wordDF.groupBy("word").count

------------------------------------------------------------------------------------------------------------------------------------------------------- 
116	what internally happens when you do spark-submit ?

https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2019/11/Internals-of-Job-Execution-In-Spark.jpg
------------------------------------------------------------------------------------------------------------------------------------------------------- 
117	 How to read binary files in spark ?

  assuming you have no delimiter, 
  an Int in Scala is 4 bytes,
  Short is 2 byte, 
  long is 8 bytes. 
  You should be able to take the bytes and convert them to the classes you want.

   import java.nio.ByteBuffer
  val result = YourRDD.map(x=>(ByteBuffer.wrap(x.take(4)).getInt,
             ByteBuffer.wrap(x.drop(4).take(2)).getShort,
             ByteBuffer.wrap(x.drop(6)).getLong))
------------------------------------------------------------------------------------------------------------------------------------------------------- 
118	how will you check if data is there or not in 6th partition in RDD ? 
==> using glom() methond
  val rdd = sc.parallelize(Seq(1,2,3,4,5,6,7,8,9,10))
  rdd.glom().collect()

  //Result
  res3: Array[Array[Int]] = Array(Array(1), Array(2), Array(3), Array(4, 5), Array(6), Array(7), Array(8), Array(9, 10))
------------------------------------------------------------------------------------------------------------------------------------------------------- 
119 what is stage n task in spark

Spark Stages : 
Whenever there is a shuffling of data over the network, Spark divides the job into multiple stages. 
Therefore, a stage is created when the shuffling of data takes place.
These stages can be either processed parallelly or sequentially depending upon the dependencies of these stages between each other. 

There are two types of stages in Spark:

1.ShuffleMapStage in Spark
2. ResultStage in Spark
------------------------------------------------------------------------------------------------------------------------------------------------------- 
120	Rank vs DenseRank
Rank - same rank to mulple records if tie, but then skips sequence. ( 1,1,3,4)
DenseRank - same rank to mulple records if tie, but then skips sequence.. dense_rank leaves no gaps in ranking sequence when there are ties.( 1,1,2,3)
------------------------------------------------------------------------------------------------------------------------------------------------------- 
121	SMB joins

Shuffle phase
Data from both datasets are read and shuffled. After the shuffle operation, records with the same keys from both datasets will end up in the same partition 
after the shuffle. Note that the entire dataset is not broadcasted with this join. This means the dataset in each partition will be in a manageable size 
after the shuffle.

Sort phase 
Records on both sides are sorted by key. Hashing and bucketing are not involved with this join.

Merge phase
A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted the merge or the join operation is stopped for an element 
as soon as a key mismatch is encountered. So a join attempt is not performed on all keys.

------------------------------------------------------------------------------------------------------------------------------------------------------- 

122	how we do GC in Spark
JVM garbage collection is problematic with large churn RDD stored by the program. To make room for new objects, Java removes the older one; it traces all the
old objects and finds the unused one. But the key point is that cost of garbage collection in Spark is proportional to a number of Java objects. Thus, it is
better to use a data structure in Spark with lesser objects. 
