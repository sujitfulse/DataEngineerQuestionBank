151	"write a program where we can filter all null values and concat an '_' with all non null values

val newDf =
  df.withColumn(
    ""NEW_COLUMN"",
    concat(
      when(col(""COL1"").isNotNull, col(""COL1"")),'-') );
      
 -------------------------------------------------------------------------------------------------------------------------------------------------------  
 
152, 153	"What are the roles and responsibilities of worker nodes in the Apache Spark cluster? Is Worker Node in Spark is same as Slave Node?

      Apache Spark follows a master/slave architecture, with one master or driver process and more than one slave or worker processes
      
      1. The master is the driver that runs the main() program where the spark context is created. 
         It then interacts with the cluster manager to schedule the job execution and perform the tasks.

      2. The worker consists of processes that can run in parallel to perform the tasks scheduled by the driver program. 
         These processes are called executors.

      Whenever a client runs the application code, the driver programs instantiates Spark Context, converts the transformations and 
      actions into logical DAG of execution. 
      This logical DAG is then converted into a physical execution plan, which is then broken down into smaller physical execution units. 
      The driver then interacts with the cluster manager to negotiate the resources required to perform the tasks of the application code. 
      The cluster manager then interacts with each of the worker nodes to understand the number of executors running in each of them.

      The role of worker nodes/executors:

      1. Perform the data processing for the application code
      2. Read from and write the data to the external sources
      3. Store the computation results in memory, or disk.
     
      Before the execution of tasks, the executors are registered with the driver program through the cluster manager, 
      so that the driver knows how many numbers of executors are running to perform the scheduled tasks. 
      The executors then start executing the tasks scheduled by the worker nodes through the cluster manager.

      Whenever any of the worker nodes fail, the tasks that are required to be performed will be automatically allocated to any other worker nodes"
------------------------------------------------------------------------------------------------------------------------------------------------------- 

    154	How to chain Dataframe transformation in Spark Transform method ?
    
  implicit classes or the Dataset#transform method can be used to chain DataFrame transformations in Spark.
  Though Dataset#transform method is preferred compared to implicit classes.

      Eg:

      val df = Seq(
        ""funny"",
        ""person""
      ).toDF(""something"")

      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }

      def withFarewell(df: DataFrame): DataFrame = {
        df.withColumn(""farewell"", lit(""goodbye""))
      }


      val weirdDf = df
        .transform(withGreeting)
        .transform(withFarewell)
    
       weirdDf.show()
      +---------+-----------+--------+
      |something|   greeting|farewell|
      +---------+-----------+--------+
      |    funny|hello world| goodbye|
      |   person|hello world| goodbye|

      Transform Method with Arguments

      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }
      def withCat(name: String)(df: DataFrame): DataFrame = {
        df.withColumn(""cats"", lit(s""$name meow""))
      }

      val niceDf = df
        .transform(withGreeting)
        .transform(withCat(""puffy""))


      Monkey Patching with Implicit Classes :
      Implicit classes can be used to add methods to existing classes. 
      The following code adds the same withGreeting() and withFarewell() methods to the DataFrame class itself.

      object BadImplicit {
        implicit class DataFrameTransforms(df: DataFrame) {
          def withGreeting(): DataFrame = {
            df.withColumn(""greeting"", lit(""hello world""))
          }
          def withFarewell(): DataFrame = {
            df.withColumn(""farewell"", lit(""goodbye""))
          }
        }
      }
      The withGreeting() and withFarewell() methods can be chained and executed as follows.

      import BadImplicit._
      val df = Seq(
        ""funny"",
        ""person""
      ).toDF(""something"")
      val hiDf = df.withGreeting().withFarewell()"
------------------------------------------------------------------------------------------------------------------------------------------------------- 
155, 162	difference between traits and abstract class ?

      Traits
      Traits are similar to interfaces in Java and are created using trait keyword.

      Abstract Class
      Abstract Class is similar to abstract classes in Java and are created using abstract keyword.

      1) Multiple inheritance : 	
      Trait supports multiple inheritance.	
      Abstract Class supports single inheritance only.
      2) Instance	
      Trait can be added to an object instance.	
      Abstract class cannot be added to an object instance.
      3) Constructor parameters	
      Trait cannot have parameters in its constructors.	
      Abstract class can have parameterised constructor.
      4) Interoperability	
      Traits are interoperable with java if they don't have any implementation.	
      Abstract classes are interoperable with java without any restriction.
      5) Stackability	
      Traits are stackable and are dynamically bound.	
      Abstract classes are not stacable and are statically bound.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

163	difference between map and foreach ? when to use a map and when to use foreach ?

      map :
      the map function in the Seq trait returns a value.
      It is designed to apply a function to every element of a collection extending the Seq trait and return a new collection.
      def map[B](f: (A) ⇒ B): Seq[B]

      foreach, in the same trait, has the following signature:
      def foreach(f: (A) ⇒ Unit): Unit

      val l: List[String] = List("a", "b", "c")
      l.map(e => e.toUpperCase(java.util.Locale.ROOT))
      l.foreach(e => println(e))
------------------------------------------------------------------------------------------------------------------------------------------------------- 

164	what is case class and when we will use a case class ?

    Scala case classes are just regular classes which are immutable by default and decomposable through pattern matching.
    It uses equal method to compare instance structurally. It does not use new keyword to instantiate object.
    All the parameters listed in the case class are public and immutable by default.
    
    case class className(parameters)  
    
    case class CaseClass(a:Int, b:Int)    
    object MainObject{  
        def main(args:Array[String]){  
            var c =  CaseClass(10,10)       // Creating object of case class  
            println("a = "+c.a)               // Accessing elements of case class  
            println("b = "+c.b)  
        }  
    }  

A case class which has no arguments is declared as case object instead of case class. case object is serializeable by default.

      trait SuperTrait  
      case class CaseClass1(a:Int,b:Int) extends SuperTrait  
      case class CaseClass2(a:Int) extends SuperTrait         // Case class  
      case object CaseObject extends SuperTrait               // Case object  
      object MainObject{  
          def main(args:Array[String]){  
              callCase(CaseClass1(10,10))  
              callCase(CaseClass2(10))  
              callCase(CaseObject)  
          }  
          def callCase(f:SuperTrait) = f match{  
              case CaseClass1(f,g)=>println("a = "+f+" b ="+g)  
              case CaseClass2(f)=>println("a = "+f)  
              case CaseObject=>println("No Argument")  
          }  
      }  



------------------------------------------------------------------------------------------------------------------------------------------------------- 

165, 166	what if partion is deleted from external table and will it give error while a select ?

167	how to improve performance of two big tables in hive ?

169	spark scala word count programm

170	What is future class in scala

176	what internally happens when you do spark-submit ?

177	 How to read binary files in spark ?

178	List hive  analytic functions you used in project 

179	how will you check if data is there or not in 6th partition in RDD ? 

183 what is stage n task in spark

184	in case one task is taking more time how will you handle --> Specutive execution.

85	Rank vs DenseRank

186	InferSchema method API

187	Broadcast join

188	Map Side Join

189	SMB joins

195	how we do GC in Spark

196	have you ever received an error "spaceout" in your datanode? what is it exactly?

199 how do you allocate buffer memory to your datanode?	how much buffer space have you allocated to your map task and reduce task in your datanode?
