<Spark Architecture and features theory question>
-----------------------------------------------------------------------------------------------------------------------------------------------------------
51	"Why Apache Spark?

	1. Hadoop MapReduce can only allow for batch processing.
	2. If we talk about stream processing only Apache Storm / S4 can perform.
	3. Again for interactive processing, we need Apache Impala / Apache Tez.
	4. While we need to perform graph processing, we opt for Neo4j / Apache Giraph.
	Therefore, No single engine can perform all the tasks together. 
	hence there was a big demand for a powerful engine that can process the data in real-time (streaming) as well as in batch mode
	Also, which can respond to sub-second and perform in-memory processing.
	In this way, Apache Spark comes in picture. It is a powerful open-source engine that offers interactive processing, real-time stream processing, 
	graph processing, in-memory processing as well as batch processing. 
	Even with very fast speed, ease of use and also standard interface at the same time."
---------------------------------------------------------------------------------------------------------------------------------------------------	
	
52	"What are the components of Apache Spark Ecosystem?

	Apache spark consists of following components
	1.Spark Core
	2.Spark SQL
	3.Spark Streaming
	4.MLlib
	5.GraphX

	Spark Core: Spark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, 
	interacting with storage systems, and more. Spark Core is also home to the API that defines resilient distributed datasets (RDDs), 
	which are Spark’s main programming abstraction. It also provides many APIs for building and manipulating these RDDS.

	Spark SQL: Spark SQL provides an interface to work with structured data.It allows querying in SQL as well as Apache Hivevariant of SQL(HQL).
	It supports many sources.

	Spark Streaming: It is spark component that enables processing of live streams of data.

	MLlib: Spark comes with common machine learning package called MLlib

	GraphX: GraphX is a library for manipulating graphs (e.g., a social network’s friend graph)and performing graph-parallel computations."
---------------------------------------------------------------------------------------------------------------------------------------------------	

53	"What is Spark Core?

	Spark Core is the fundamental unit of the whole Spark project. It provides all sort of functionalities like task dispatching, scheduling, and 
	input-output operations etc.Spark makes use of Special data structure known as RDD (Resilient Distributed Dataset). 
	It is the home for API that defines and manipulate the RDDs. Spark Core is distributed execution engine with all the functionality attached on its top.
	For example, MLlib, SparkSQL, GraphX, Spark Streaming. Thus, allows diverse workload on single platform. All the basic functionality of Apache Spark 
	Like in-memory computation, fault tolerance, memory management, monitoring, task scheduling is provided by Spark Core.
	Apart from this Spark also provides the basic connectivity with the data sources. For example, HBase, Amazon S3, HDFS etc."
---------------------------------------------------------------------------------------------------------------------------------------------------	
54	What is Catalyst Optimizer in spark ?

  At the core of Spark SQL is the Catalyst optimizer, which leverages advanced programming language features 
  (e.g. Scala’s pattern matching and quasi quotes) in a novel way to build an extensible query optimizer.
  Catalyst is based on functional programming constructs in Scala and designed with these key two purposes:
    -Easily add new optimization techniques and features to Spark SQL
    -Enable external developers to extend the optimizer (e.g. adding data source specific rules, support for new data types, etc.)
  Catalyst contains a general library for representing trees and applying rules to manipulate them. 
  On top of this framework, it has libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules 
  that handle different phases of query execution: 
  analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode.
---------------------------------------------------------------------------------------------------------------------------------------------------	
55	"In what modes Spark can run as individual ? "What are the different methods to run Spark over Apache Hadoop?

	1. Local mode – There is no resource manager in local mode. This mode is used for test the spark application in test environment 
	where we do not want to eat the resources and want to run applications faster.
	Here everything run on single JVM.

	2. Distributed / Cluster modes:

	We can run spark on distributed manner with master-slave architecture.
	There will be multiple worker nodes in each cluster and cluster manager will be allocating the resources to each worked node.

	Spark can be deployed in distributed cluster in 3 ways.

	1. Standalone:

	In standalone mode spark itself handle the resource allocation, their won’t be any separate cluster manager. 
	Spark allocated the CPU and memory to worker nodes based on the resource availability.

	2. YARN :

	Here, YARN will be used as cluster manager. YARN distribution will be mainly used when spark running with other Hadoop components 
	like MR in Cloudera or HortonWorks Distribution. YARN is a combination of Resource Manager and Node Manager.

	Resource manager has Scheduler and Application manager.
	Scheduler: Scheduler allocate resources to various running application
	Application Manager: Manages all application across all nodes.

	Node Manager contains Application master and container.
	The container is the place where actual work happens.
	Application master negotiate resources from Resource manager.

	3. Mesos:

	Mesos is used in large scala production deployments. In meson distribution, all the resources available in the cluster across all nodes will be 
	clubbed together and  dynamic sharing of resources will be done.
	Meson master, slave, and framework are the three components of mess.
	master-provides fault tolerance
	slave- actually does the resource allocation
	framework-help the application to request for resources"
---------------------------------------------------------------------------------------------------------------------------------------------------	
56	What is the role of Akka in Spark?
   Answer: Akka in Spark helps in the scheduling process. It helps the workers and masters to send and receive messages for 
   workers for tasks and master requests for registering."
---------------------------------------------------------------------------------------------------------------------------------------------------	

57	"What is SparkContext in Apache Spark?

	A SparkContext is a client of Spark’s execution environment and it acts as the master of the Spark application. 
	SparkContext sets up internal services and establishes a connection to a Spark execution environment. 
	One can create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext stops) 
	after the creation of SparkContext. 
	Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one.

	In Spark shell, a special interpreter-aware SparkContext is already created for the user, in the variable called sc.

	Few functionalities which SparkContext offers are: 
	1. We can get the current status of a Spark application like configuration, app name.
	2. We can set Configuration like master URL, default logging level.
	3. One can create Distributed Entities like RDDs."
---------------------------------------------------------------------------------------------------------------------------------------------------	

58	"What is SparkSession in Apache Spark?

	Starting from Apache Spark 2.0, Spark Session is the new entry point for Spark applications.
	Prior to 2.0, SparkContext was the entry point for spark jobs. RDD was one of the main APIs then, and it was created and manipulated 
	using Spark Context. 
	For every other APIs, different contexts were required – For SQL, SQL Context was required; For Streaming, Streaming Context was required; 
	For Hive, Hive Context was required.

	But from 2.0, RDD along with DataSet and its subset DataFrame APIs are becoming the standard APIs and are a basic unit of data abstraction
	in Spark. All of the user defined code will be written and evaluated against the DataSet and DataFrame APIs as well as RDD.

	So, there is a need for a new entry point build for handling these new APIs, which is why Spark Session has been introduced.
	Spark Session also includes all the APIs available in different contexts – Spark Context, SQL Context, Streaming Context, Hive Context"
---------------------------------------------------------------------------------------------------------------------------------------------------	

59	"SparkSession vs SparkContext in Apache Spark.

	Prior to Spark 2.0.0 sparkContext was used as a channel to access all spark functionality.
	sparkConf is required to create the spark context object, which stores configuration parameter like appName (to identify your spark driver), 
	application, number of core and memory size of executor running on worker node. 
	In order to use APIs of SQL, HIVE, and Streaming, separate contexts need to be created.
	SPARK 2.0.0 onwards, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming 
	Spark with DataFrame and Dataset APIs. In order to use APIs of SQL, HIVE, and Streaming, no need to create separate contexts as 
	sparkSession includes all the APIs."
---------------------------------------------------------------------------------------------------------------------------------------------------	

60	"How to create SparkConf ?

	val conf = new SparkConf().
	setAppName(“RetailDataAnalysis”).
	setMaster(“spark://master:7077”).
	set(“spark.executor.memory”, “2g”)

	creation of sparkContext:
	val sc = new SparkContext(conf)"
---------------------------------------------------------------------------------------------------------------------------------------------------	

61	"How to create a spark session ?

	val spark = SparkSession
	.builder
	.appName(“WorldBankIndex”)
	.getOrCreate()

	Configuring properties:
	spark.conf.set(“spark.sql.shuffle.partitions”, 6)
	spark.conf.set(“spark.executor.memory”, “2g”)"
---------------------------------------------------------------------------------------------------------------------------------------------------	

62	"How can we create RDD in Apache Spark?

	Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.
	Including user-defined classes, RDDs may contain any type of Python, Java, or Scala objects.

	In 3 ways we can create RDD in Apache Spark:
	1. Through distributing collection of objects
	2. By loading an external dataset
	3. From existing Apache Spark RDDs

	1. Using parallelized collection

	RDDs are generally created by parallelizing an existing collection
	i.e. by taking an existing collection in the program and passing
	it to SparkContext’s parallelize() method.

	scala > val data = Array(1,2,3,4,5)
	scala > val dataRDD = sc.parallelize (data)
	scala > dataRDD.count

	2. External Datasets

	In Spark, a distributed dataset can be formed from any data source supported by Hadoop.

	val dataRDD = spark.read.textFile(“/Spark/Posts.xml”).rdd

	3. Creating RDD from existing RDD

	Transformation is the way to create an RDD from already existing RDD.

	Transformation acts as a function that intakes an RDD and produces another resultant RDD.
	The input RDD does not get changed,
	Some of the operations applied on RDD are: filter, Map, FlatMap

	val dataRDD = spark.read.textFile(“Spark/Posts.xml”).rdd

	val resultRDD = data.filter{line => {line.trim().startsWith(“<row”)}
	}"

---------------------------------------------------------------------------------------------------------------------------------------------------	
63	"Explain Accumulator in Spark.

	Accumulator is a shared variable in Apache Spark, used to aggregating information across the cluster.
	In other words, aggregating information / values from worker nodes back to the driver program. 


	Why Accumulator :
	When we use a function inside the operation like map(), filter() etc these functions can use the variables which defined outside 
	these function scope in the driver program.When we submit the task to cluster, each task running on the cluster gets a new copy
	of these variables and updates from these variable do not propagated back to the driver program.Accumulator lowers this restriction.


	Use Case :

	Meaning count the no. of blank lines from the input file, no. of bad packets from network during session, 
	during Olympic data analysis we have to find age where we said (age != ‘NA’) in SQL query in short finding bad / corrupted records.

	Example :

	scala> val record = spark.read.textFile(""/home/hdadmin/wc-data-blanklines.txt"")
	val emptylines = sc.accumulator(0)
	val processdata = record.flatMap(x =>
							{
								if(x == """")
									 emptylines += 1
								 x.split("" "")
							 })

	scala> processdata.collect           //will show non-empty lines 
	scala> println(“No. of Empty Lines : ” + emptylines.value)  //show empty lines 
	
	Please note that, task(s) on worker nodes can not access the value property of accumulator so for in context of task(s), 
	accumulator is write-only variable.
	The value() property of accumulator is available only in the driver program.
	We can also count the no. of blank lines with the help of transformation/actions but for that, 
	we need an extra operation but with the help of accumulator, we can count the no. of blank lines (or events in broader terms) as 
	we load /process our data.
	//Imp point

	Please keep in mind that, for accumulators which are used in action operation, Spark applies each task’s update only once. 
	Hence we want to reliable result from accumulator, in that case, we must use it inside actions regardless of failure of multiple evaluations.
	If we use the accumulator in transformation, in that case, this guarantee does not exist. 
	An accumulator update within transformation can occur more than once."

---------------------------------------------------------------------------------------------------------------------------------------------------

64	"How to identify that given operation is Transformation/Action in your program?

	In order to identify the operation, one need to look at the return type of an operation.
	If operation returns a new RDD in that case an operation is ‘Transformation’
	If operation returns any other type than RDD in that case an operation is ‘Action’"

---------------------------------------------------------------------------------------------------------------------------------------------------
65	 Can we trigger automated clean-ups in Spark?
   Answer: Yes, we can trigger automated clean-ups in Spark to handle the accumulated metadata. 
   It can be done by setting the parameters, namely, “spark.cleaner.ttl.” 
   
   Question: What is another method than “Spark.cleaner.ttl” to trigger automated clean-ups in Spark?
   Answer: Another method than “Spark.clener.ttl” to trigger automated clean-ups in Spark is by dividing the long-running jobs into 
   different batches and writing the intermediary results on the disk."
---------------------------------------------------------------------------------------------------------------------------------------------------	

66	"What are the types of Apache Spark transformation?

	there are fundamentally two types of transformations:

	1. Narrow transformation –
	While talking about Narrow transformation, all the elements which are required to compute the records in single partition reside 
	in the single partition of parent RDD. To calculate the result, a limited subset of partition is used. 
	This Transformation are the result of map(), filter().

	2. Wide Transformations – 
	Wide transformation means all the elements that are required to compute the records in the single partition may live in many partitions 
	of parent RDD. Partitions may reside in many different partitions of parent RDD. T
	his Transformation is a result of groupbyKey() and reducebyKey()."
---------------------------------------------------------------------------------------------------------------------------------------------------	

67	"Explain the RDD properties.

	1. List or Set of partitions.
	2. List of dependencies on other (parent) RDD
	3. A function to compute each partition
	Below operations are used for optimization during execution.
	4. Optional preferred location [i.e. block location of an HDFS file] [it’s about data locality] 
	5. Optional partitioned info [i.e. Hash-Partition for Key/Value pair –> When data shuffled how data will be traveled]"

---------------------------------------------------------------------------------------------------------------------------------------------------	

68	"What is lineage graph in Apache Spark?

	When we apply a different transformation on RDD it creates RDD Linage graph. It is a new RDD from already existing RDDs.
	It is the dependencies graph between the existing and the new RDD formed. 
	The need of RDD lineage graph arrives when we want to compute new RDD or if we want to recover the lost data from the lost persisted RDD.
	You can check lineage between two RDDs using rdd0.toDebugString. 
	This gives back you the lineage graph from current rdd to all the previous dependencies of RDDs.
	See below. Whenever you see “+-” symbol from the toDebugString output, it means there will be next stage from the next operation onwards. 
	This is indicates to identify that how many stage are created."
---------------------------------------------------------------------------------------------------------------------------------------------------	

69	Explain the terms  Spark Partitions and Partitioners.

	Partition in Spark is similar to split in HDFS. 
	A partition in Spark is a logical division of data stored on a node in the cluster. 
	They are the basic units of parallelism in Apache Spark. R
	DDs are a collection of partitions. When some actions are executed, a task is launched per partition.

	By default, partitions are automatically created by the framework. However, the number of partitions in Spark are configurable to suit the needs.
	For the number of partitions, if spark.default.parallelism is set, then we should use the value from SparkContext defaultParallelism, 
	otherwise we should use the max number of upstream partitions. Unless spark.default.parallelism is set, the number of partitions will be the 
	same as that of the largest upstream RDD, as this would least likely cause an out-of-memory errors.
	
	A partitioner is an object that defines how the elements in a key-value pair RDD are partitioned by key, maps each key to a partition ID
	from 0 to numPartitions – 1. It captures the data distribution at the output. With the help of partitioner, the scheduler can optimize 
	the future operations. The contract of partitioner ensures that records for a given key have to reside on a single partition.
	We should choose a partitioner to use for a cogroup-like operations. If any of the RDDs already has a partitioner, 
	we should choose that one. Otherwise, we use a default HashPartitioner.

	There are three types of partitioners in Spark :
	a) Hash Partitioner b) Range Partitioner c) Custom Partitioner
	Hash – Partitioner : Hash- partitioning attempts to spread the data evenly across various partitions based on the key.
	Range – Partitioner : In Range- Partitioning method , tuples having keys with same range will appear on the same machine.

	RDDs can be created with specific partitioning in two ways :
	i) Providing explicit partitioner by calling partitionBy method on an RDD
	ii) Applying transformations that return RDDs with specific partitioners"

---------------------------------------------------------------------------------------------------------------------------------------------------	
70	"What is the role of Spark Driver in spark applications?

	In easy terms, the driver in Spark creates SparkContext, connected to a given Spark Master.
	It conjointly delivers the RDD graphs to Master, wherever the standalone cluster manager runs.
	It splits a Spark application into tasks and schedules them to run on executors.
	A driver is where the task scheduler lives and spawns tasks across workers.
	A driver coordinates workers and overall execution of tasks.
	Whenever a client runs the application code, the driver programs instantiates Spark Context, 
	converts the transformations and actions into logical DAG of execution.
	This logical DAG is then converted into a physical execution plan, which is then broken down into smaller physical execution units.
	The driver then interacts with the cluster manager to negotiate the resources required to perform the tasks of the application code. 
	The cluster manager then interacts with each of the worker nodes to understand the number of executors running in each of them."
---------------------------------------------------------------------------------------------------------------------------------------------------	

71	"What is Spark DataFrames?


	DataFrame consists of two words data and frame, means data has to be fit in some kind of frame.
	We can understand a frame as a schema of the relational database.

	In Spark, DataFrame is a collection of distributed data over the network with some schema. 
	We can understand it as the data formatted as row/column manner. 
	DataFrame can be created from Hive data, JSON file, CSV, Structured data or raw data that can be framed in structured data.
	We can also create a DataFrame from RDD if some schema can be applied on that RDD.
	Temporary view or table can also be created from DataFrame as it has data and schema.
	We can also run SQL query on created table/view to get the faster result.
	It is also evaluated lazily (Lazy Evaluation) for better resource utilization."
---------------------------------------------------------------------------------------------------------------------------------------------------
72	"How can data transfer be minimized when working with Apache Spark?


	In Spark, Data Transfer can be reduced by avoiding operation which results in data shuffle.
	Avoid operations like repartition and coalesce, ByKey operations like groupByKey and reduceByKey, and join operations like cogroup and join.

	Spark Shared Variables help in reducing data transfer. There two types for shared variables-Broadcast variable and Accumulator.

	Broadcast variable
	Accumulator
---------------------------------------------------------------------------------------------------------------------------------------------------
73	"What is Spark Dataset?

	A Dataset is an immutable collection of objects, those are mapped to a relational schema. 
	They are strongly-typed in nature. 
	There is an encoder, at the core of the Dataset API. That Encoder is responsible for converting between JVM objects and tabular representation. 
	By using Spark’s internal binary format, the tabular representation is stored that allows to carry out operations on serialized data and 
	improves memory utilization.
	Datasets are ""lazy"", i.e. computations are only triggered when an action is invoked. 
	Internally, a Dataset represents a logical plan that describes the computation required to produce the data. 
	When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a physical plan for efficient execution in a 
	parallel and distributed manner. To explore the logical plan as well as optimized physical plan, use the explain.function.
 
 	val people = spark.read.parquet(""..."")
   	val department = spark.read.parquet(""..."")

 	  people.filter(""age > 30"")
    	 .join(department, people(""deptId"") === department(""id""))
    	 .groupBy(department(""name""), people(""gender""))
    	 .agg(avg(people(""salary"")), max(people(""age"")))

	To efficiently support domain-specific objects, an Encoder is required. 
	The encoder maps the domain specific type T to Spark's internal type system.
	For example, given a class Person with two fields, name (string) and age (int), an encoder is used to tell Spark to generate 
	code at runtime to serialize the Person object into a binary structure. 
	This binary structure often has much lower memory footprint as well as are optimized for efficiency in data processing 
	(e.g. in a columnar format). To understand the internal binary representation for data, use the schema function."

---------------------------------------------------------------------------------------------------------------------------------------------------

74	"What are the advantages of datasets in spark?

	Static typing feature of Dataset, a developer can catch errors at compile time 
	Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan providing the 
	space and speed efficiency."
---------------------------------------------------------------------------------------------------------------------------------------------------

75	"What is the difference between DAG and Lineage?

	Lineage graph
	As we know, that whenever a series of transformations are performed on an RDD, they are not evaluated immediately, 
	but lazily(Lazy Evaluation). When a new RDD has been created from an existing RDD, that new RDD contains a pointer to the parent RDD. 
	Similarly, all the dependencies between the RDDs will be logged in a graph, rather than the actual data. This graph is called the lineage graph.

	Now coming to DAG,

	Directed Acyclic Graph(DAG)
	DAG in Apache Spark is a combination of Vertices as well as Edges. 
	In DAG vertices represent the RDDs and the edges represent the Operation to be applied on RDD. 
	Every edge in DAG is directed from earlier to later in a sequence.
	When we call anAction, the created DAG is submitted to DAG Scheduler which further splits the graph into the stages of the task."
---------------------------------------------------------------------------------------------------------------------------------------------------
		
76	What is the difference between Caching and Persistence in Apache Spark?

	Cache is a synonym of Persist with MEMORY_ONLY storage level(i.e) using Cache technique we can save intermediate results 
	in memory only when needed.

	Persist marks an RDD for persistence using storage level which can be 
	MEMORY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2"
---------------------------------------------------------------------------------------------------------------------------------------------------

77	"What are the limitations of Apache Spark?

	1. No File Management System
	2. No support for Real-Time Processing
	3. Manual Optimization
	4. Window Criteria:	Spark does not support record based window criteria. It only has time-based window criteria.
	5. Expensive : 	when we want cost-efficient processing of big data In-memory capability can become a bottleneck as keeping data in memory i
---------------------------------------------------------------------------------------------------------------------------------------------------

78	"Different Running Modes of Apache Spark


	We can launch spark application in four modes:

	1) Local Mode (local[*],local,local[2]…etc)
	-> When you launch spark-shell without control/configuration argument, It will launch in local mode
	spark-shell –master local[1]
	-> spark-submit –class com.df.SparkWordCount SparkWC.jar local[1]

	2) Spark Standalone cluster manger:
	-> spark-shell –master spark://hduser:7077
	-> spark-submit –class com.df.SparkWordCount SparkWC.jar spark://hduser:7077

	3) Yarn mode (Client/Cluster mode):
	-> spark-shell –master yarn or
	(or)
	->spark-shell –master yarn –deploy-mode client

	Above both commands are same.
	To launch spark application in cluster mode, we have to use spark-submit command. We cannot run yarn-cluster mode via spark-shell because when we run spark application, driver program will be running as part application master container/process. So it is not possible to run cluster mode via spark-shell.
	-> spark-submit –class com.df.SparkWordCount SparkWC.jar yarn-client
	-> spark-submit –class com.df.SparkWordCount SparkWC.jar yarn-cluster

	4) Mesos mode:
	-> spark-shell –master mesos://HOST:5050"
	
---------------------------------------------------------------------------------------------------------------------------------------------------
79	"What is write ahead log(journaling) in Spark?

	There are two types of failures in any Apache Spark job – Either the driver failure or the worker failure.

	When any worker node fails, the executor processes running in that worker node will be killed, and the tasks which were scheduled on that worker
	node will be automatically moved to any of the other running worker nodes, and the tasks will be accomplished.

	When the driver or master node fails, all of the associated worker nodes running the executors will be killed, along with the data in each of 
	the executors’ memory. 
	
	To overcome this data loss scenario, Write Ahead Logging (WAL) has been introduced in Apache Spark 1.2. With WAL enabled,
	the intention of the operation is first noted down in a log file, such that if the driver fails and is restarted, 
	the noted operations in that log file can be applied to the data. For sources that read streaming data, like Kafka or Flume, 
	receivers will be receiving the data, and those will be stored in the executor’s memory. 
	With WAL enabled, these received data will also be stored in the log files.

	WAL can be enabled by performing the below:

	1. Setting the checkpoint directory, by using streamingContext.checkpoint(path)
	2. Enabling the WAL logging, by setting spark.stream.receiver.WriteAheadLog.enable to True."
	
---------------------------------------------------------------------------------------------------------------------------------------------------
80	"Explain catalyst query optimizer in Apache Spark.
	
	This optimizer is based on functional programming construct in Scala.
	Catalyst Optimizer supports both rule-based and cost-based optimization. 
	In rule-based optimization the rule based optimizer use set of rule to determine how to execute the query. 
	While the cost based optimization finds the most suitable way to carry out SQL statement. 
	In cost-based optimization, multiple plans are generated using rules and then their cost is computed.

	Catalyst optimizer makes use of standard features of Scala programming like pattern matching.
	In the depth, Catalyst contains the tree and the set of rules to manipulate the tree. T
	here are specific libraries to process relational queries. 

	There are various rule sets which handle different phases of query execution like analysis, query optimization, physical planning,
	and code generation to compile parts of queries to Java bytecode."
---------------------------------------------------------------------------------------------------------------------------------------------------

81	"What are shared variables in Apache Spark?

        Shared variables are nothing but the variables that can be used in parallel operations. 
	By default, when Apache Spark runs a function in parallel as a set of tasks on different nodes, 
	it ships a copy of each variable used in the function to each task.
	Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. 
	Spark supports two types of shared variables: 
	broadcast variables can be used to cache a value in memory on all nodes.
	accumulators are variables that are only “added” to, such as counters and sums."
---------------------------------------------------------------------------------------------------------------------------------------------------

82	"How does Apache Spark handles accumulated Metadata?

	Metadata accumulates on the driver as consequence of shuffle operations. It becomes particularly tedious during long-running jobs.
	To deal with the issue of accumulating metadata, there are two options:

	First, set the spark.cleaner.ttl parameter to trigger automatic cleanups. However, this will vanish any persisted RDDs.
	The other solution is to simply split long-running jobs into batches and write intermediate results to disk. 
	This facilitates a fresh environment for every batch and don’t have to worry about metadata build-up."
---------------------------------------------------------------------------------------------------------------------------------------------------
83	"what is breeze?

	Breeze is a collection of libraries for numerical computing and machine learning."
 ---------------------------------------------------------------------------------------------------------------------------------------------------
84	"List commonly used Machine Learning Algorithm.

	(1) Supervised Learning Algorithm
	(2) Unsupervised Learning Algorithm
	(3) Reinforcement Learning Algorithm

	> Most commonly used Machine Learning Algorithm are as follows :
	(1) Linear Regression
	(2) Logistic Regression
	(3) Decision Tree
	(4) K-Means
	(5) KNN
	(6) SVM
	(7) Random Forest
	(8) Naïve Bayes
	(9) Dimensionality Reduction Algorithm
	(10) Gradient Boost and Adaboost"
---------------------------------------------------------------------------------------------------------------------------------------------------

85	"What is the difference between DSM and RDD?

	On the basis of several features, the difference between RDD and DSM is:

	i. Read

	RDD – The read operation in RDD is either coarse-grained or fine-grained. 
	      Coarse-grained meaning we can transform the whole dataset but not an individual element on the dataset. 
	      While fine-grained means we can transform individual element on the dataset.
	DSM – The read operation in Distributed shared memory is fine-grained.

	ii. Write

	RDD – The write operation in RDD is coarse-grained.
	DSM – The Write operation is fine grained in distributed shared system.

	Fault-Recovery Mechanism
	RDD – By using lineage graph at any moment, the lost data can be easily recovered in Spark RDD. 
	Therefore, for each transformation, new RDD is formed. As RDDs are immutable in nature, hence, it is easy to recover.
	DSM – Fault tolerance is achieved by a checkpointing technique which allows applications to roll back to a recent checkpoint 
	rather than restarting.


	Straggler Mitigation

	Stragglers, in general, are those that take more time to complete than their peers. 
	This could happen due to many reasons such as load imbalance, I/O blocks, garbage collections, etc.
	An issue with the stragglers is that when the parallel computation is followed by synchronizations such as reductions that 
	causes all the parallel tasks to wait for others.

	RDD – It is possible to mitigate stragglers by using backup task, in RDDs.
	DSM – To achieve straggler mitigation, is quite difficult.

	vi. Behavior if not enough RAM

	RDD – As there is not enough space to store RDD in RAM, therefore, the RDDs are shifted to disk.
	DSM – If the RAM runs out of storage, the performance decreases, in this type of systems."

---------------------------------------------------------------------------------------------------------------------------------------------------
86	 What is Speculative Execution in Apache Spark?

	The Speculative task in Apache Spark is task that runs slower than the rest of the task in the job.
	It is health check process that verifies the task is speculated, meaning the task that runs slower than the median of successfully 
	completed task in the task sheet. 
	Such tasks are submitted to another worker. 
	It runs the new copy in parallel rather than shutting down the slow task.


	Spark Property >> Default Value >> Description
	spark.speculation >> false >> enables ( true ) or disables ( false ) speculative execution of tasks.
	spark.speculation.interval >> 100ms >> The time interval to use before checking for speculative tasks.
	spark.speculation.multiplier >> 1.5 >> How many times slower a task is than the median to be for speculation.
	spark.speculation.quantile >> 0.75 >> The percentage of tasks that has not finished yet at which to start speculation."
---------------------------------------------------------------------------------------------------------------------------------------------------
