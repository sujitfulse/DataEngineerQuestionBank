1	What is difference between order by and sort by ? 
  They are NOT the SAME.
  The SORT BY clause is used to return the result rows sorted within each partition in the user specified order. 
  When there is more than one partition SORT BY may return result that is partially ordered.
  Reference :https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sortby.html
  The ORDER BY clause is used to return the result rows in a sorted manner in the user specified order. 
  Unlike the SORT BY clause, this clause guarantees a total order in the output.
  Reference : https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-orderby.html
----------------------------------------------------------------------------------------------------------------------------
2	What are the important requirements For Spark Submit
    spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  <application-arguments>
----------------------------------------------------------------------------------------------------------------------------
3	What is mean by Broadcast Variable in spark ? 
Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. 
They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. 
Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.

import org.apache.spark.sql.SparkSession
object BroadcastExample extends App{

  val spark = SparkSession.builder()
    .appName("SparkByExamples.com")
    .master("local")
    .getOrCreate()

  val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
  val countries = Map(("USA","United States of America"),("IN","India"))

  val broadcastStates = spark.sparkContext.broadcast(states)
  val broadcastCountries = spark.sparkContext.broadcast(countries)

  val data = Seq(("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  )

  val columns = Seq("firstname","lastname","country","state")
  import spark.sqlContext.implicits._
  val df = data.toDF(columns:_*)

  val df2 = df.map(row=>{
    val country = row.getString(2)
    val state = row.getString(3)

    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (row.getString(0),row.getString(1),fullCountry,fullState)
  }).toDF(columns:_*)

  df2.show(false)
}

----------------------------------------------------------------------------------------------------------------------------
 
4	How to utilize hive buckets in spark ?

Bucketing is a partitioning technique that can improve performance in certain data transformations by avoiding data shuffling and sorting. 
The general idea of bucketing is to partition, and optionally sort, the data based on a subset of columns while it is written out (a one-time cost), 
Bucketing can enable faster joins (i.e. single stage sort merge join), 
the ability to short circuit in FILTER operation if the file is pre-sorted over the column in a filter predicate, and it supports quick data sampling.

Hive bucketed tables are supported from Spark 2.3 onwards. 
Spark normally disallow users from writing outputs to Hive Bucketed tables.
Setting hive.enforce.bucketing=false and hive.enforce.sorting=false will allow you to save to Hive Bucketed tables.

 Spark still won't produce bucketed data as per Hive's bucketing guarantees, 
 but will allow writes IFF user wishes to do so without caring about bucketing guarantees. 
 Ability to create bucketed tables will enable adding test cases to Spark while pieces are being added to Spark have it support hive bucketing 

//enable Hive support when creating/configuring the spark session
val spark = SparkSession.builder().enableHiveSupport().getOrCreate()

//register DF as view that can be used with SparkSQL
val testDF = Seq((1, "a"),(2, "b"),(3, "c")).toDF("number", "letter")
testDF.createOrReplaceTempView("testDF")

//create Hive table, can also be done manually, e.g. via Hive CLI
val createTableSQL = "CREATE TABLE testTable (number int, letter string) CLUSTERED BY number INTO 1 BUCKETS STORED AS PARQUET"
spark.sql(createTableSQL)

//load data from DF into Hive, output parquet files will be bucketed and readable by Hive
spark.sql("INSERT INTO testTable SELECT * FROM testDF")

https://community.cloudera.com/t5/Support-Questions/Hive-bucketed-table-from-Spark-2-3/td-p/221572
https://medium.com/@deepa.account/comparison-between-spark-and-hive-bucketing-827ec41fe58d
----------------------------------------------------------------------------------------------------------------------------

5	Write a Spark Scala Code to Create Tempview from a Text file
val textFile = sc.textFile("hdfs://localhost:9000/user/input.txt")
textFile.createOrReplaceTempView("testDF")
----------------------------------------------------------------------------------------------------------------------------

6	We have Two Input files: .csv and .parquet with policy and insured details. You have to join based on some command column and run some business logic to hive table Using data frame.
 write sparksql this was supposed to be done"

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

object checkDFSchema extends App {
  val cc = new SparkConf;
  val sc = new SparkContext(cc)
  val sparkSession = SparkSession.builder().enableHiveSupport().getOrCreate()
  //First option for creating hive table through dataframe 
  val DF = sparkSession.sql("select * from salary")
  DF.createOrReplaceTempView("tempTable")
  sparkSession.sql("Create table yourtable as select * form tempTable")
  //Second option for creating hive table from schema
  val oldDFF = sparkSession.sql("select * from salary")
  //Generate the schema out of dataframe  
  val schema = oldDFF.schema
  //Generate RDD of you data 
  val rowRDD = sc.parallelize(Seq(Row(100, "a", 123)))
  //Creating new DF from data and schema 
  val newDFwithSchema = sparkSession.createDataFrame(rowRDD, schema)
  newDFwithSchema.createOrReplaceTempView("tempTable")
  sparkSession.sql("create table FinalTable AS select * from tempTable")
}
https://stackoverflow.com/questions/42261701/how-to-create-hive-table-from-spark-data-frame-using-its-schema
----------------------------------------------------------------------------------------------------------------------------

7	How the folder structure is constructed in ur project in Eclipse IDE
src/main/java ==> java code 
src/test/java ==> test cases
Maven Depdencies ==> all jars
pom.xml
----------------------------------------------------------------------------------------------------------------------------

8	How do u access the global variable in ur spark code ? 
 you can use broadcast mechanism to access global variable. but it is immutable.
 Spark is built on the principle of immutability, in fact any distributed framework works by leveraging the concepts of immutability. 
 so you can not modify global variable.
 https://stackoverflow.com/questions/36401537/how-to-define-a-global-read-write-variables-in-spark
----------------------------------------------------------------------------------------------------------------------------
9	What are the starting lines you write in spark code.
// declair spark configuration object 
// declair spark context 
// build spark session 
e.g. 
  val cc = new SparkConf;
  val sc = new SparkContext(cc)
  val sparkSession = SparkSession.builder().getOrCreate()
  val DF = sparkSession.sql("select * from salary") 
----------------------------------------------------------------------------------------------------------------------------  
10	Explode function in Spark
convert collection (array/ maps) columns to rows in order to process in Spark effectively.
Spark explode functions -> 
A) explode ==> This will ignore elements that have null or empty datatypes. 
B) explode_outer ==> This will not ignore elements that have null or empty datatypes. 
C) posexplode  ==> This will ignore elements that have null or empty datatypes. it will add one more column to show position i.e. index number in array/map
D) posexplode_outer==> This will not ignore elements that have null or empty datatypes. it will add one more column to show position i.e. index number in array/map
https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/
----------------------------------------------------------------------------------------------------------------------------  

11	What is case class in spark
Case classes uses reflection to generate the schema of an RDD that contains specific types of objects. 
The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. 
The case class defines the schema of the table. 
The names of the arguments to the case class are read using reflection and they become the names of the columns.
Case classes can also be nested or contain complex types such as Sequences or Arrays. 
This RDD can be implicitly be converted to a DataFrame and then registered as a table. Tables can be used in subsequent SQL statements.
example : 
data 
1201, satish, 25
1202, krishna, 28
1203, amith, 39
1204, javed, 23
1205, prudvi, 23

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
scala> import sqlContext.implicts._
scala> case class Employee(id: Int, name: String, age: Int)
scala> val empl=sc.textFile("employee.txt")
.map(_.split(","))
.map(e⇒ employee(e(0).trim.toInt,e(1), e(2).trim.toInt))
.toDF()
scala> empl.registerTempTable("employee")
scala> val allrecords = sqlContext.sql("SELeCT * FROM employee")
https://www.tutorialspoint.com/spark_sql/inferring_schema_using-reflection.htm#:~:text=The%20Scala%20interface%20for%20Spark,the%20names%20of%20the%20columns.
----------------------------------------------------------------------------------------------------------------------------

12	How to process nested json or nested array in spark
    Step 1: Load JSON data into Spark Dataframe using API
    Step 2: Explode Array datasets in Spark Dataframe.
    Step 3: Fetch each order using GetItem on Explored columns.
    Step 4: Repeat step 1 to 4 , untill we flatten all the object to column lelel.
 
 import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

 val ordersDf = spark.read.format("json")
                         .option("inferSchema", "true")
                         .option("multiLine", "true")
                         .load("/FileStore/tables/orders_sample_datasets.json")
 import org.apache.spark.sql.functions._
 val parseOrdersDf = ordersDf.withColumn("orders", explode($"datasets"))      
// Step 3: Fetch Each Order using getItem on explode column
 val parseOrdersDf = parseOrdersDf.withColumn("customerId", $"orders".getItem("customerId"))
                             .withColumn("orderId", $"orders".getItem("orderId"))
                             .withColumn("orderDate", $"orders".getItem("orderDate"))
                             .withColumn("orderDetails", $"orders".getItem("orderDetails"))
                             .withColumn("shipmentDetails", $"orders".getItem("shipmentDetails"))                  
                         
   https://bigdataprogrammers.com/read-nested-json-in-spark-dataframe/
----------------------------------------------------------------------------------------------------------------------------
13	How to select few columns from an rdd and not converting it to a df
   - use case class or struct to define DF column names.
   - use rdd.map() and then choose selected column only.
----------------------------------------------------------------------------------------------------------------------------
14	If the data type is not correct, then How to enforce the data type on a column after the df is created

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.DataType
import org.apache.spark.sql.types.{StringType, StructType,IntegerType,StructField}

val simpleData = Seq(Row("James ","","Smith","36636","M",3000),
    Row("Michael ","Rose","","40288","M",4000),
    Row("Robert ","","Williams","42114","M",4000),
    Row("Maria ","Anne","Jones","39192","F",4000),
    Row("Jen","Mary","Brown","","F",-1)
  )

val simpleSchema = StructType(Array(
    StructField("firstname",StringType,true),
    StructField("middlename",StringType,true),
    StructField("lastname",StringType,true),
    StructField("id", StringType, true),
    StructField("gender", StringType, true),
    StructField("salary", IntegerType, true)
  ))

 val df = spark.createDataFrame(spark.sparkContext.parallelize(simpleData),simpleSchema)
 df.printSchema()
 df.show()
 // NOTE : if data type is not matched then it will fail 
  https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/
----------------------------------------------------------------------------------------------------------------------------
15	How to read parquet file in spark

val parqDF = spark.read.parquet("/tmp/output/people.parquet")
df.write.parquet("/tmp/output/people.parquet")
----------------------------------------------------------------------------------------------------------------------------
16	Fold vs Left fold vs right fold difference in Scala

foldLeft starts on the left side—the first item—and iterates to the right; FoldLeft is a iterative solution.
foldRight starts on the right side—the last item—and iterates to the left. FoldRight: is a recursive solution 
fold goes in no particular order.

val numList = List(1, 2, 3, 4, 5)

val r1 = numList.par.fold(0)((acc, value) => {
  println("adding accumulator=" + acc + ", value=" + value + " => " + (acc + value))
  acc + value
})
println("fold(): " + r1)
println("#######################")

/*
 * You can see from the output that,
 * fold process the elements of parallel collection in parallel
 * So it is parallel not linear operation.
 * 
 * adding accumulator=0, value=4 => 4
 * adding accumulator=0, value=3 => 3
 * adding accumulator=0, value=1 => 1
 * adding accumulator=0, value=5 => 5
 * adding accumulator=4, value=5 => 9
 * adding accumulator=0, value=2 => 2
 * adding accumulator=3, value=9 => 12
 * adding accumulator=1, value=2 => 3
 * adding accumulator=3, value=12 => 15
 * fold(): 15
 */

val r2 = numList.par.foldLeft(0)((acc, value) => {
  println("adding accumulator=" + acc + ", value=" + value + " => " + (acc + value))
  acc + value
})
println("foldLeft(): " + r2)
println("#######################")
/*
 * You can see that foldLeft
 * picks elements from left to right.
 * It means foldLeft does sequence operation
 * 
 * adding accumulator=0, value=1 => 1
 * adding accumulator=1, value=2 => 3
 * adding accumulator=3, value=3 => 6
 * adding accumulator=6, value=4 => 10
 * adding accumulator=10, value=5 => 15
 * foldLeft(): 15
 * #######################
 */

// --> Note in foldRight second arguments is accumulated one.
val r3 = numList.par.foldRight(0)((value, acc) => {
 println("adding value=" + value + ", acc=" + acc + " => " + (value + acc))
  acc + value
})
println("foldRight(): " + r3)
println("#######################")

/*
 * You can see that foldRight
 * picks elements from right to left.
 * It means foldRight does sequence operation.
 * 
 * adding value=5, acc=0 => 5
 * adding value=4, acc=5 => 9
 * adding value=3, acc=9 => 12
 * adding value=2, acc=12 => 14
 * adding value=1, acc=14 => 15
 * foldRight(): 15
 * #######################
 */
----------------------------------------------------------------------------------------------------------------------------

17	Map vs flat map difference in scala.
map returns only one element.
flatMap returns a list of elements (0 or more) as an iterator.

val data=sc.textFile("sparkdata.txt")  
val splitdata = data.flatMap(line => line.split(" "));  
var splitdataPair = splitdata.map((_, 1))
val reducedata = splitdataPair.reduceByKey(_+_); 
reducedata.saveAsTextFile("my_spark_shell_wc_output")
----------------------------------------------------------------------------------------------------------------------------
18	What is currying function in Scala ? 

Currying transforms a function that takes multiple parameters into a chain of functions, each taking a single parameter.

*example : 
def strcat(s1: String)(s2: String) = s1 + s2
OR 
def strcat(s1: String) = (s2: String) => s1 + s2
Calling func : strcat("foo")("bar")
https://www.tutorialspoint.com/scala/currying_functions.htm

*Partially applied Currying funtion : 
result = f(x)(y)(z)
can be called as 
f1 = f(x)
f2 = f1(y)
result = f2(z)

*Advantages of Currying Function in Scala :
a) One benefit is that Scala currying makes creating anonymous functions easier.
b) Scala Currying also makes it easier to pass around a function as a first-class object. You can keep applying parameters when you find them.
----------------------------------------------------------------------------------------------------------------------------

19	Closure functions in scala and what are their benefits.
Scala Closures are functions which uses one or more free variables and the return value of this function is dependent of these variable. 
The free variables are defined outside of the Closure Function and is not included as a parameter of this function. 
So the difference between a closure function and a normal function is the free variable. 
A free variable is any kind of variable which is not defined within the function and not passed as the parameter of the function. 
A free variable is not bound to a function with a valid value.

A closure function can further be classified into pure and impure functions, depending on the type of the free variable. 
-If we give the free variable a type var then the variable tends to change the value any time throughout the entire code and thus may result in changing the value.
Thus this closure is a impure function. 
-On the other-hand if we declare the free variable of the type val then the value of the variable remains constant and thus making the closure function a pure one.
// Addition of two numbers with
// Scala closure

// Creating object
object GFG
{
	// Main method
	def main(args: Array[String])
	{
		println( "Final_Sum(1) value = " + sum(1))
		println( "Final_Sum(2) value = " + sum(2))
		println( "Final_Sum(3) value = " + sum(3))
	}
		
	var a = 4
	
	// define closure function
	val sum = (b:Int) => b + a
}

Output : 
Final_Sum(1) value = 5
Final_Sum(2) value = 6
Final_Sum(3) value = 7
https://www.geeksforgeeks.org/scala-closures/#:~:text=Scala%20Closures%20are%20functions%20which,a%20parameter%20of%20this%20function.

*The benefit of Using Closures Functions
a) A major benefit of having closure function is the concept of data encapsulation plus data persistence. 
b) Since the variables defined have a scope and if they are defined inside a function they will have a local scope, but with the help of closure function,
we have defined a global variable and can use it inside the function also.
c) we can have those variables that are available even when the function task is finished.
https://www.educba.com/scala-closure/
----------------------------------------------------------------------------------------------------------------------------

20 Explain  avro file in spark
a) Apache Avro is an open-source, row-based, data serialization and data exchange framework for Hadoop projects, originally developed by databricks .
it is mostly used in Apache Spark especially for Kafka-based data pipelines. When Avro data is stored in a file, its schema is stored with it, 
b) so that files may be processed later by any program.
c) It serializes data in a compact binary format and schema is in JSON format that defines the field names and data types.
d) It is similar to Thrift and Protocol Buffers, but does not require the code generation as it’s data always accompanied by a schema that 
permits full processing of that data without code generation. This is one of the great advantages compared with other serialization systems.

*Apache Avro Advantages
a) Supports complex data structures like Arrays, Map, Array of map and map of array elements.
b) A compact, binary serialization format which provides fast while transferring data.
c) row-based data serialization system.
d) Support multi-languages, meaning data written by one language can be read by different languages. Simple integration with dynamic languages.
e) Code generation is not required to read or write data files.

*<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-avro_2.11</artifactId>
    <version>2.4.0</version>
</dependency>

* Reading Avro Partition Data
val personDF= spark.read.format("avro").load("person.avro")
personDF.write.format("avro").save("person.avro")


spark.sqlContext.sql("CREATE TEMPORARY VIEW PERSON USING avro OPTIONS (path \"person.avro\")")
spark.sqlContext.sql("SELECT * FROM PERSON").show()


*Reading Avro Partition Data
spark.read
      .format("avro")
      .load("person_partition.avro")
      .where(col("dob_year") === 2010)
      .show()

*Writing Avro Partition Data
df.write
  .partitionBy("dob_year","dob_month")
  .format("avro")
  .save("person_partition.avro")
----------------------------------------------------------------------------------------------------------------------------

21	what are different spark distributions ?
-- HDP 
-- CDP
----------------------------------------------------------------------------------------------------------------------------

22	difference between spark 1.6 and spark 2.0

23	How to read a nested JSON data in spark

24	How to convert rdd to data frame

25	what is struct type

26	is it possible to define array field in struct

27	what is the datatype of array field in struct

28	is it possible to convert a df into rdd

29	what is output of df.rdd

30	which ide to develop spark code

31	how u deploy the code to production

32	How to use the same spark jar for different variables inputs

33	how to load the data thru spark where few records are updates and few are new ?

34	Hive and spark optimizations

35	how to replace null values with some other value or discard the rows with null values in spark

36	how to force datatype checks on column level in scala

37	How to process a log file in spark and store

38	Spark architecture

39	what are Serde properties

40	Hive context in spark

41	what id RDD

42	what are the properties of RDD

43	Higher order function? and its advantages?

44	Why you used Scala for Spark ?

45	How do you do packaging (managing packages)

46	How do you add dependencies for your project/application?

47	Why do you need dependencies?

48	pom.xml <- will things work if we rename pom.xml

49	what are minimum mandatory imports that are required for scala application

50	Explain the Difference between group by, order by, cluster by & distribute by with example ..??
