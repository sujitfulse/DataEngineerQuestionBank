1	What is difference between order by and sort by ? 
  They are NOT the SAME.
  The SORT BY clause is used to return the result rows sorted within each partition in the user specified order. 
  When there is more than one partition SORT BY may return result that is partially ordered.
  Reference :https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sortby.html
  The ORDER BY clause is used to return the result rows in a sorted manner in the user specified order. 
  Unlike the SORT BY clause, this clause guarantees a total order in the output.
  Reference : https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-orderby.html
----------------------------------------------------------------------------------------------------------------------------
2	What are the important requirements For Spark Submit
    spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  <application-arguments>
----------------------------------------------------------------------------------------------------------------------------
3	What is mean by Broadcast Variable in spark ? 
Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. 
They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. 
Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.

import org.apache.spark.sql.SparkSession
object BroadcastExample extends App{

  val spark = SparkSession.builder()
    .appName("SparkByExamples.com")
    .master("local")
    .getOrCreate()

  val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
  val countries = Map(("USA","United States of America"),("IN","India"))

  val broadcastStates = spark.sparkContext.broadcast(states)
  val broadcastCountries = spark.sparkContext.broadcast(countries)

  val data = Seq(("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  )

  val columns = Seq("firstname","lastname","country","state")
  import spark.sqlContext.implicits._
  val df = data.toDF(columns:_*)

  val df2 = df.map(row=>{
    val country = row.getString(2)
    val state = row.getString(3)

    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (row.getString(0),row.getString(1),fullCountry,fullState)
  }).toDF(columns:_*)

  df2.show(false)
}

----------------------------------------------------------------------------------------------------------------------------
 
4	How to utilize hive buckets in spark ?

Bucketing is a partitioning technique that can improve performance in certain data transformations by avoiding data shuffling and sorting. 
The general idea of bucketing is to partition, and optionally sort, the data based on a subset of columns while it is written out (a one-time cost), 
Bucketing can enable faster joins (i.e. single stage sort merge join), 
the ability to short circuit in FILTER operation if the file is pre-sorted over the column in a filter predicate, and it supports quick data sampling.

Hive bucketed tables are supported from Spark 2.3 onwards. 
Spark normally disallow users from writing outputs to Hive Bucketed tables.
Setting hive.enforce.bucketing=false and hive.enforce.sorting=false will allow you to save to Hive Bucketed tables.

 Spark still won't produce bucketed data as per Hive's bucketing guarantees, 
 but will allow writes IFF user wishes to do so without caring about bucketing guarantees. 
 Ability to create bucketed tables will enable adding test cases to Spark while pieces are being added to Spark have it support hive bucketing 

//enable Hive support when creating/configuring the spark session
val spark = SparkSession.builder().enableHiveSupport().getOrCreate()

//register DF as view that can be used with SparkSQL
val testDF = Seq((1, "a"),(2, "b"),(3, "c")).toDF("number", "letter")
testDF.createOrReplaceTempView("testDF")

//create Hive table, can also be done manually, e.g. via Hive CLI
val createTableSQL = "CREATE TABLE testTable (number int, letter string) CLUSTERED BY number INTO 1 BUCKETS STORED AS PARQUET"
spark.sql(createTableSQL)

//load data from DF into Hive, output parquet files will be bucketed and readable by Hive
spark.sql("INSERT INTO testTable SELECT * FROM testDF")

https://community.cloudera.com/t5/Support-Questions/Hive-bucketed-table-from-Spark-2-3/td-p/221572
https://medium.com/@deepa.account/comparison-between-spark-and-hive-bucketing-827ec41fe58d
----------------------------------------------------------------------------------------------------------------------------

5	Write a Spark Scala Code to Create Tempview from a Text file
val textFile = sc.textFile("hdfs://localhost:9000/user/input.txt")
textFile.createOrReplaceTempView("testDF")
----------------------------------------------------------------------------------------------------------------------------

6	We have Two Input files: .csv and .parquet with policy and insured details. You have to join based on some command column and run some business logic to hive table Using data frame.
 write sparksql this was supposed to be done"

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

object checkDFSchema extends App {
  val cc = new SparkConf;
  val sc = new SparkContext(cc)
  val sparkSession = SparkSession.builder().enableHiveSupport().getOrCreate()
  //First option for creating hive table through dataframe 
  val DF = sparkSession.sql("select * from salary")
  DF.createOrReplaceTempView("tempTable")
  sparkSession.sql("Create table yourtable as select * form tempTable")
  //Second option for creating hive table from schema
  val oldDFF = sparkSession.sql("select * from salary")
  //Generate the schema out of dataframe  
  val schema = oldDFF.schema
  //Generate RDD of you data 
  val rowRDD = sc.parallelize(Seq(Row(100, "a", 123)))
  //Creating new DF from data and schema 
  val newDFwithSchema = sparkSession.createDataFrame(rowRDD, schema)
  newDFwithSchema.createOrReplaceTempView("tempTable")
  sparkSession.sql("create table FinalTable AS select * from tempTable")
}
https://stackoverflow.com/questions/42261701/how-to-create-hive-table-from-spark-data-frame-using-its-schema
----------------------------------------------------------------------------------------------------------------------------

7	How the folder structure is constructed in ur project in Eclipse IDE
src/main/java ==> java code 
src/test/java ==> test cases
Maven Depdencies ==> all jars
pom.xml
----------------------------------------------------------------------------------------------------------------------------

8	How do u access the global variable in ur spark code ? 
 you can use broadcast mechanism to access global variable. but it is immutable.
 Spark is built on the principle of immutability, in fact any distributed framework works by leveraging the concepts of immutability. 
 so you can not modify global variable.
 https://stackoverflow.com/questions/36401537/how-to-define-a-global-read-write-variables-in-spark
----------------------------------------------------------------------------------------------------------------------------

9	What r the starting lines u write in spark code

10	Explode function in Spark

11	What is case class in spark

12	How to process nested json or nested array in hive and spark

13	How to select few columns from an rdd and not converting it to a df

14	If the data type is not correct, then How to enforce the data type on a column after the df is created

15	How to read parquet file in spark

16	Left fold..right fold diff. in Scala

17	Map..flat map diff.

18	What is currying function in Scala

19	Closure functions and what are their benefits.

20	which version of spark and hive

21	what is the distribution u r using

22	difference between spark 1.6 and spark 2.0

23	How to read a nested JSON data in spark

24	How to convert rdd to data frame

25	what is struct type

26	is it possible to define array field in struct

27	what is the datatype of array field in struct

28	is it possible to convert a df into rdd

29	what is output of df.rdd

30	which ide to develop spark code

31	how u deploy the code to production

32	How to use the same spark jar for different variables inputs

33	how to load the data thru spark where few records are updates and few are new ?

34	Hive and spark optimizations

35	how to replace null values with some other value or discard the rows with null values in spark

36	how to force datatype checks on column level in scala

37	How to process a log file in spark and store

38	Spark architecture

39	what are Serde properties

40	Hive context in spark

41	what id RDD

42	what are the properties of RDD

43	Higher order function? and its advantages?

44	Why you used Scala for Spark ?

45	How do you do packaging (managing packages)

46	How do you add dependencies for your project/application?

47	Why do you need dependencies?

48	pom.xml <- will things work if we rename pom.xml

49	what are minimum mandatory imports that are required for scala application

50	Explain the Difference between group by, order by, cluster by & distribute by with example ..??
