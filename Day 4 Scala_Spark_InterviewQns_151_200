151	"write a program where we can filter all null values and concat an '_' with all non null values

val newDf =   df.withColumn("NEW_COLUMN",concat(when(col(""COL1"").isNotNull, col(""COL1"")),'-'));
      
 -------------------------------------------------------------------------------------------------------------------------------------------------------  
 
152	What are the roles and responsibilities of worker nodes in the Apache Spark cluster? Is Worker Node in Spark is same as Slave Node?

      Apache Spark follows a master/slave architecture, with one master or driver process and more than one slave or worker processes
      
      1. The master is the driver that runs the main() program where the spark context is created. 
         It then interacts with the cluster manager to schedule the job execution and perform the tasks.

      2. The worker consists of processes that can run in parallel to perform the tasks scheduled by the driver program. 
         These processes are called executors.

      Whenever a client runs the application code, the driver programs instantiates Spark Context, converts the transformations and 
      actions into logical DAG of execution. 
      This logical DAG is then converted into a physical execution plan, which is then broken down into smaller physical execution units. 
      The driver then interacts with the cluster manager to negotiate the resources required to perform the tasks of the application code. 
      The cluster manager then interacts with each of the worker nodes to understand the number of executors running in each of them.

      The role of worker nodes/executors:

      1. Perform the data processing for the application code
      2. Read from and write the data to the external sources
      3. Store the computation results in memory, or disk.
     
      Before the execution of tasks, the executors are registered with the driver program through the cluster manager, 
      so that the driver knows how many numbers of executors are running to perform the scheduled tasks. 
      The executors then start executing the tasks scheduled by the worker nodes through the cluster manager.

      Whenever any of the worker nodes fail, the tasks that are required to be performed will be automatically allocated to any other worker nodes"
------------------------------------------------------------------------------------------------------------------------------------------------------- 

153	How to chain Dataframe transformation in Spark Transform method ?
    
  implicit classes or the Dataset#transform method can be used to chain DataFrame transformations in Spark.
  Though Dataset#transform method is preferred compared to implicit classes.

      Eg:

      val df = Seq("funny","person").toDF("something")

      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }

      def withFarewell(df: DataFrame): DataFrame = {
        df.withColumn(""farewell"", lit(""goodbye""))
      }


      val weirdDf = df
        .transform(withGreeting)
        .transform(withFarewell)
    
       weirdDf.show()
      +---------+-----------+--------+
      |something|   greeting|farewell|
      +---------+-----------+--------+
      |    funny|hello world| goodbye|
      |   person|hello world| goodbye|

      Transform Method with Arguments
-----------------------------
      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }
      def withCat(name: String)(df: DataFrame): DataFrame = {
        df.withColumn(""cats"", lit(s""$name meow""))
      }

      val niceDf = df
        .transform(withGreeting)
        .transform(withCat(""puffy""))
---------------------------

      Monkey Patching with Implicit Classes :
      Implicit classes can be used to add methods to existing classes. 
      The following code adds the same withGreeting() and withFarewell() methods to the DataFrame class itself.

      object BadImplicit {
        implicit class DataFrameTransforms(df: DataFrame) {
          def withGreeting(): DataFrame = {
            df.withColumn(""greeting"", lit(""hello world""))
          }
          def withFarewell(): DataFrame = {
            df.withColumn(""farewell"", lit(""goodbye""))
          }
        }
      }
      The withGreeting() and withFarewell() methods can be chained and executed as follows.

      import BadImplicit._
      val df = Seq("funny","person").toDF(""something"")
      val hiDf = df.withGreeting().withFarewell()"
------------------------------------------------------------------------------------------------------------------------------------------------------- 
154	difference between traits and abstract class ?

      Traits
      Traits are similar to interfaces in Java and are created using trait keyword.

      Abstract Class
      Abstract Class is similar to abstract classes in Java and are created using abstract keyword.

      1) Multiple inheritance : 	
      Trait supports multiple inheritance.	
      Abstract Class supports single inheritance only.
      2) Instance	
      Trait can be added to an object instance.	
      Abstract class cannot be added to an object instance.
      3) Constructor parameters	
      Trait cannot have parameters in its constructors.	
      Abstract class can have parameterised constructor.
      4) Interoperability	
      Traits are interoperable with java if they don't have any implementation.	
      Abstract classes are interoperable with java without any restriction.
      5) Stackability	
      Traits are stackable and are dynamically bound.	
      Abstract classes are not stacable and are statically bound.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

155	difference between map and foreach ? when to use a map and when to use foreach ?

      map :
      the map function in the Seq trait returns a value.
      It is designed to apply a function to every element of a collection extending the Seq trait and return a new collection.
      def map[B](f: (A) ⇒ B): Seq[B]

      foreach, in the same trait, has the following signature:
      def foreach(f: (A) ⇒ Unit): Unit

      val l: List[String] = List("a", "b", "c")
      l.map(e => e.toUpperCase(java.util.Locale.ROOT))
      l.foreach(e => println(e))
------------------------------------------------------------------------------------------------------------------------------------------------------- 

156	what is case class and when we will use a case class ?

    Scala case classes are just regular classes which are immutable by default and decomposable through pattern matching.
    It uses equal method to compare instance structurally. It does not use new keyword to instantiate object.
    All the parameters listed in the case class are public and immutable by default.
    
    case class className(parameters)  
    
    case class CaseClass(a:Int, b:Int)    
    object MainObject{  
        def main(args:Array[String]){  
            var c =  CaseClass(10,10)       // Creating object of case class  
            println("a = "+c.a)               // Accessing elements of case class  
            println("b = "+c.b)  
        }  
    }  

A case class which has no arguments is declared as case object instead of case class. case object is serializeable by default.

      trait SuperTrait  
      case class CaseClass1(a:Int,b:Int) extends SuperTrait  
      case class CaseClass2(a:Int) extends SuperTrait         // Case class  
      case object CaseObject extends SuperTrait               // Case object  
      object MainObject{  
          def main(args:Array[String]){  
              callCase(CaseClass1(10,10))  
              callCase(CaseClass2(10))  
              callCase(CaseObject)  
          }  
          def callCase(f:SuperTrait) = f match{  
              case CaseClass1(f,g)=>println("a = "+f+" b ="+g)  
              case CaseClass2(f)=>println("a = "+f)  
              case CaseObject=>println("No Argument")  
          }  
      }  
------------------------------------------------------------------------------------------------------------------------------------------------------- 

moved
------------------------------------------------------------------------------------------------------------------------------------------------------- 

160	spark scala word count programm

RDD : 
 val text = sc.textFile("mytextfile.txt") 
 val counts = text.flatMap(line => line.split(" ") ).map(word => (word,1)).reduceByKey(_+_) counts.collect 

DataFrame :
val df = sqlContext.read.text("README.md")
val wordsDF = df.select(split(df("value")," ").alias("words"))
val wordDF = wordsDF.select(explode(wordsDF("words")).alias("word"))
val wordCountDF = wordDF.groupBy("word").count

------------------------------------------------------------------------------------------------------------------------------------------------------- 
161	What is future class in scala
  Futures allows performing many operations in parallel– in an efficient and non-blocking way.
  A Future is a placeholder object for a value that may not yet exist. 
  Generally, the value of the Future is supplied concurrently and can subsequently be used. 
  Composing concurrent tasks in this way tends to result in faster, asynchronous, non-blocking parallel code.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

162	what internally happens when you do spark-submit ?

https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2019/11/Internals-of-Job-Execution-In-Spark.jpg
------------------------------------------------------------------------------------------------------------------------------------------------------- 
163	 How to read binary files in spark ?

  assuming you have no delimiter, 
  an Int in Scala is 4 bytes,
  Short is 2 byte, 
  long is 8 bytes. 
  You should be able to take the bytes and convert them to the classes you want.

   import java.nio.ByteBuffer
  val result = YourRDD.map(x=>(ByteBuffer.wrap(x.take(4)).getInt,
             ByteBuffer.wrap(x.drop(4).take(2)).getShort,
             ByteBuffer.wrap(x.drop(6)).getLong))
------------------------------------------------------------------------------------------------------------------------------------------------------- 

moved
------------------------------------------------------------------------------------------------------------------------------------------------------- 

165	how will you check if data is there or not in 6th partition in RDD ? 
==> using glom() methond
  val rdd = sc.parallelize(Seq(1,2,3,4,5,6,7,8,9,10))
  rdd.glom().collect()

  //Result
  res3: Array[Array[Int]] = Array(Array(1), Array(2), Array(3), Array(4, 5), Array(6), Array(7), Array(8), Array(9, 10))
------------------------------------------------------------------------------------------------------------------------------------------------------- 
166 what is stage n task in spark

Spark Stages : 
Whenever there is a shuffling of data over the network, Spark divides the job into multiple stages. 
Therefore, a stage is created when the shuffling of data takes place.
These stages can be either processed parallelly or sequentially depending upon the dependencies of these stages between each other. 

There are two types of stages in Spark:

1.ShuffleMapStage in Spark
2. ResultStage in Spark
------------------------------------------------------------------------------------------------------------------------------------------------------- 
167	Rank vs DenseRank
Rank - same rank to mulple records if tie, but then skips sequence. ( 1,1,3,4)
DenseRank - same rank to mulple records if tie, but then skips sequence.. dense_rank leaves no gaps in ranking sequence when there are ties.( 1,1,2,3)
------------------------------------------------------------------------------------------------------------------------------------------------------- 
168	SMB joins

Shuffle phase
Data from both datasets are read and shuffled. After the shuffle operation, records with the same keys from both datasets will end up in the same partition 
after the shuffle. Note that the entire dataset is not broadcasted with this join. This means the dataset in each partition will be in a manageable size 
after the shuffle.

Sort phase 
Records on both sides are sorted by key. Hashing and bucketing are not involved with this join.

Merge phase
A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted the merge or the join operation is stopped for an element 
as soon as a key mismatch is encountered. So a join attempt is not performed on all keys.

------------------------------------------------------------------------------------------------------------------------------------------------------- 

169	how we do GC in Spark
JVM garbage collection is problematic with large churn RDD stored by the program. To make room for new objects, Java removes the older one; it traces all the
old objects and finds the unused one. But the key point is that cost of garbage collection in Spark is proportional to a number of Java objects. Thus, it is
better to use a data structure in Spark with lesser objects. 
------------------------------------------------------------------------------------------------------------------------------------------------------- 

170	What is currying function in Scala ? 

Currying transforms a function that takes multiple parameters into a chain of functions, each taking a single parameter.

*example : 
def strcat(s1: String)(s2: String) = s1 + s2

*Partially applied Currying funtion : 
result = f(x)(y)(z)
can be called as 
f1 = f(x)
f2 = f1(y)
result = f2(z)

object Curry 
{ 
    def add(a: Int) = (b: Int) => a + b; 
    def main(args: Array[String]) 
    { 
      // first way to call 
         println(add(20)(20)); 
    
     // alternative way to call   
        val add2 = add(10);
        println(add2(20)); 
    } 
} 

*Advantages of Currying Function in Scala :
a) One benefit is that Scala currying makes creating anonymous functions easier.
b) Scala Currying also makes it easier to pass around a function as a first-class object. You can keep applying parameters when you find them.
----------------------------------------------------------------------------------------------------------------------------

171	Closure functions in scala and what are their benefits.

Difference between a closure function and a normal function is the free variable. 
A free variable is any kind of variable which is not defined within the function and not passed as the parameter of the function. 
A free variable is not bound to a function with a valid value.
Scala Closures are functions which uses one or more free variables and the return value of this function is dependent of these variable. 


A closure function can further be classified into pure(val variables) and impure functions ( var variables), depending on the type of the free variable. 
-If we give the free variable a type var then the variable tends to change the value any time throughout the entire code and thus may result in changing the value.
Thus this closure is a impure function. 
-On the other-hand if we declare the free variable of the type val then the value of the variable remains constant and thus making the closure function a pure one.
// Addition of two numbers with
// Scala closure

// Creating object
object GFG
{
	// Main method
	def main(args: Array[String])
	{
		println( "Final_Sum(1) value = " + sum(1))
		println( "Final_Sum(2) value = " + sum(2))
		println( "Final_Sum(3) value = " + sum(3))
	}
		
	var a = 4
	
	// define closure function
	val sum = (b:Int) => b + a
}

Output : 
Final_Sum(1) value = 5
Final_Sum(2) value = 6
Final_Sum(3) value = 7
https://www.geeksforgeeks.org/scala-closures/#:~:text=Scala%20Closures%20are%20functions%20which,a%20parameter%20of%20this%20function.

*The benefit of Using Closures Functions
a) A major benefit of having closure function is the concept of data encapsulation plus data persistence. 
b) Since the variables defined have a scope and if they are defined inside a function they will have a local scope, but with the help of closure function,
we have defined a global variable and can use it inside the function also.
c) we can have those variables that are available even when the function task is finished.
https://www.educba.com/scala-closure/
-----------------------------------------------------------------------------------------------------------------------------------------------------

172 higher order function with examples
173 Nested functions 
174 Partially applied functions
