51	"Why Apache Spark?

	1. Hadoop MapReduce can only allow for batch processing.
	2. If we talk about stream processing only Apache Storm / S4 can perform.
	3. Again for interactive processing, we need Apache Impala / Apache Tez.
	4. While we need to perform graph processing, we opt for Neo4j / Apache Giraph.
	Therefore, No single engine can perform all the tasks together. 
	hence there was a big demand for a powerful engine that can process the data in real-time (streaming) as well as in batch mode
	Also, which can respond to sub-second and perform in-memory processing.
	In this way, Apache Spark comes in picture. It is a powerful open-source engine that offers interactive processing, real-time stream processing, 
	graph processing, in-memory processing as well as batch processing. 
	Even with very fast speed, ease of use and also standard interface at the same time."
---------------------------------------------------------------------------------------------------------------------------------------------------	
	
52	"What are the components of Apache Spark Ecosystem?

	Apache spark consists of following components
	1.Spark Core
	2.Spark SQL
	3.Spark Streaming
	4.MLlib
	5.GraphX

	Spark Core: Spark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, 
	interacting with storage systems, and more. Spark Core is also home to the API that defines resilient distributed datasets (RDDs), 
	which are Spark’s main programming abstraction. It also provides many APIs for building and manipulating these RDDS.

	Spark SQL: Spark SQL provides an interface to work with structured data.It allows querying in SQL as well as Apache Hivevariant of SQL(HQL).
	It supports many sources.

	Spark Streaming: It is spark component that enables processing of live streams of data.

	MLlib: Spark comes with common machine learning package called MLlib

	GraphX: GraphX is a library for manipulating graphs (e.g., a social network’s friend graph)and performing graph-parallel computations."
---------------------------------------------------------------------------------------------------------------------------------------------------	

53	"What is Spark Core?

Spark Core is the fundamental unit of the whole Spark project. It provides all sort of functionalities like task dispatching, scheduling, and 
input-output operations etc.Spark makes use of Special data structure known as RDD (Resilient Distributed Dataset). 
It is the home for API that defines and manipulate the RDDs. Spark Core is distributed execution engine with all the functionality attached on its top.
For example, MLlib, SparkSQL, GraphX, Spark Streaming. Thus, allows diverse workload on single platform. All the basic functionality of Apache Spark 
Like in-memory computation, fault tolerance, memory management, monitoring, task scheduling is provided by Spark Core.
Apart from this Spark also provides the basic connectivity with the data sources. For example, HBase, Amazon S3, HDFS etc."
---------------------------------------------------------------------------------------------------------------------------------------------------	

54	"How is Apache Spark better than Hadoop?

	1. Processing the data in memory which is not possible in Hadoop
	2. Processing the data that is in batch, iterative, interactive & streaming i.e. Real Time mode. Whereas Hadoop processes only in batch mode.
	3. Spark is faster because it reduces the number of disk read-write operations due to its virtue of storing intermediate data in memory. 
		Whereas in Hadoop MapReduce intermediate output which is output of Map() is always written on local hard disk
	4. Apache Spark is easy to program as it has hundreds of high-level operators with RDD (Resilient Distributed Dataset)
	5. Apache Spark code is compact due compared to Hadoop MapReduce. Use of Scala makes it very short, reduces programming efforts. 
	6. Spark application running in Hadoop clusters is up to 10 times faster on disk than Hadoop MapReduce."
---------------------------------------------------------------------------------------------------------------------------------------------------	

55,56	"In what modes Spark can run as individual ? "What are the different methods to run Spark over Apache Hadoop?

	1. Local mode – There is no resource manager in local mode. This mode is used for test the spark application in test environment 
	where we do not want to eat the resources and want to run applications faster.
	Here everything run on single JVM.

	2. Distributed / Cluster modes:

	We can run spark on distributed manner with master-slave architecture.
	There will be multiple worker nodes in each cluster and cluster manager will be allocating the resources to each worked node.

	Spark can be deployed in distributed cluster in 3 ways.

	1. Standalone:

	In standalone mode spark itself handle the resource allocation, their won’t be any separate cluster manager. 
	Spark allocated the CPU and memory to worker nodes based on the resource availability.

	2. YARN :

	Here, YARN will be used as cluster manager. YARN distribution will be mainly used when spark running with other Hadoop components 
	like MR in Cloudera or HortonWorks Distribution. YARN is a combination of Resource Manager and Node Manager.

	Resource manager has Scheduler and Application manager.
	Scheduler: Scheduler allocate resources to various running application
	Application Manager: Manages all application across all nodes.

	Node Manager contains Application master and container.
	The container is the place where actual work happens.
	Application master negotiate resources from Resource manager.

	3. Mesos:

	Mesos is used in large scala production deployments. In meson distribution, all the resources available in the cluster across all nodes will be 
	clubbed together and  dynamic sharing of resources will be done.
	Meson master, slave, and framework are the three components of mess.
	master-provides fault tolerance
	slave- actually does the resource allocation
	framework-help the application to request for resources"
---------------------------------------------------------------------------------------------------------------------------------------------------	

57	"What is Apache Mesos ?

	Spark runs on top of Mesos. Mesos is a cluster manager system that provides efficient resource isolation across distributed applications, 
	including MPI and Hadoop. Mesos enables fine-grained sharing that allows a Spark job to dynamically take advantage of the ideal resources 
	in the cluster during execution. This results in performance improvements, especially for long-running Spark jobs.

---------------------------------------------------------------------------------------------------------------------------------------------------	

58	"What is SparkContext in Apache Spark?

	A SparkContext is a client of Spark’s execution environment and it acts as the master of the Spark application. 
	SparkContext sets up internal services and establishes a connection to a Spark execution environment. 
	One can create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext stops) 
	after the creation of SparkContext. 
	Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one.

	In Spark shell, a special interpreter-aware SparkContext is already created for the user, in the variable called sc.

	Few functionalities which SparkContext offers are: 
	1. We can get the current status of a Spark application like configuration, app name.
	2. We can set Configuration like master URL, default logging level.
	3. One can create Distributed Entities like RDDs."
---------------------------------------------------------------------------------------------------------------------------------------------------	

59	"What is SparkSession in Apache Spark?

	Starting from Apache Spark 2.0, Spark Session is the new entry point for Spark applications.
	Prior to 2.0, SparkContext was the entry point for spark jobs. RDD was one of the main APIs then, and it was created and manipulated 
	using Spark Context. 
	For every other APIs, different contexts were required – For SQL, SQL Context was required; For Streaming, Streaming Context was required; 
	For Hive, Hive Context was required.

	But from 2.0, RDD along with DataSet and its subset DataFrame APIs are becoming the standard APIs and are a basic unit of data abstraction
	in Spark. All of the user defined code will be written and evaluated against the DataSet and DataFrame APIs as well as RDD.

	So, there is a need for a new entry point build for handling these new APIs, which is why Spark Session has been introduced.
	Spark Session also includes all the APIs available in different contexts – Spark Context, SQL Context, Streaming Context, Hive Context"
---------------------------------------------------------------------------------------------------------------------------------------------------	

60	"SparkSession vs SparkContext in Apache Spark.

	Prior to Spark 2.0.0 sparkContext was used as a channel to access all spark functionality.
	sparkConf is required to create the spark context object, which stores configuration parameter like appName (to identify your spark driver), 
	application, number of core and memory size of executor running on worker node. 
	In order to use APIs of SQL, HIVE, and Streaming, separate contexts need to be created.
	SPARK 2.0.0 onwards, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming 
	Spark with DataFrame and Dataset APIs. In order to use APIs of SQL, HIVE, and Streaming, no need to create separate contexts as 
	sparkSession includes all the APIs."
---------------------------------------------------------------------------------------------------------------------------------------------------	

61	"How to create SparkConf ?

	val conf = new SparkConf().
	setAppName(“RetailDataAnalysis”).
	setMaster(“spark://master:7077”).
	set(“spark.executor.memory”, “2g”)

	creation of sparkContext:
	val sc = new SparkContext(conf)"
---------------------------------------------------------------------------------------------------------------------------------------------------	

62	"How to create a spark session ?

	val spark = SparkSession
	.builder
	.appName(“WorldBankIndex”)
	.getOrCreate()

	Configuring properties:
	spark.conf.set(“spark.sql.shuffle.partitions”, 6)
	spark.conf.set(“spark.executor.memory”, “2g”)"
---------------------------------------------------------------------------------------------------------------------------------------------------	

63	" What are the abstractions of Apache Spark?

	1. RDD:
	An RDD refers to Resilient Distributed Datasets. RDDs are Read-only partition collection of records. 
	It is Spark’s core abstraction and also a fundamental data structure of Spark. It offers to conduct in-memory computations on large clusters. 

	2. DataFrames:
	It is a Dataset organized into named columns. DataFrames are equivalent to the table in a relational database or data frame in R /Python. In other words, we can say it is a relational table with good optimization technique. It is an immutable distributed collection of data. Allowing higher-level abstraction, it allows developers to impose a structure onto a distributed collection of data,. 
	
	3. Spark Streaming:
	It is a Spark’s core extension, which allows Real-time stream processing From several sources. For example Flume and Kafka. To offer a unified, continuous DataFrame abstraction that can be used for interactive and batch queries these two sources work together. It offers scalable, high-throughput and fault-tolerant processing. For more detailed insights on Spark Streaming. refer link: Spark Streaming Tutorial for Beginners

	4. GraphX
	It is one more example of specialized data abstraction. It enables developers to analyze social networks. Also, other graphs alongside Excel-like two-dimensional data. For more detailed insights on GaphX."
---------------------------------------------------------------------------------------------------------------------------------------------------	

64	"How can we create RDD in Apache Spark?

Each dataset in RDD is divided into logical partitions,
which may be computed on different nodes of the cluster.
Including user-defined classes, RDDs may contain any type of Python, Java, or Scala objects.

In 3 ways we can create RDD in Apache Spark:
1. Through distributing collection of objects
2. By loading an external dataset
3. From existing Apache Spark RDDs

1. Using parallelized collection

RDDs are generally created by parallelizing an existing collection
i.e. by taking an existing collection in the program and passing
it to SparkContext’s parallelize() method.

scala > val data = Array(1,2,3,4,5)
scala > val dataRDD = sc.parallelize (data)
scala > dataRDD.count

2. External Datasets

In Spark, a distributed dataset can be formed from any data source supported by Hadoop.

val dataRDD = spark.read.textFile(“F:/BigData/DataFlair/Spark/Posts.xml”).rdd

3. Creating RDD from existing RDD

Transformation is the way to create an RDD from already existing RDD.

Transformation acts as a function that intakes an RDD and produces another resultant RDD.
The input RDD does not get changed,
Some of the operations applied on RDD are: filter, Map, FlatMap

val dataRDD = spark.read.textFile(“F:/Mritunjay/BigData/DataFlair/Spark/Posts.xml”).rdd

val resultRDD = data.filter{line => {line.trim().startsWith(“<row”)}
}"

---------------------------------------------------------------------------------------------------------------------------------------------------	

65	"Explain the term paired RDD in Apache Spark


Paired RDD is a distributed collection of data with the key-value pair. It is a subset of Resilient Distributed Dataset. So it has all the feature of RDD and some new feature for the key-value pair. There are many transformation operations available for Paired RDD. These operations on Paired RDD are very useful to solve many use cases that require sorting, grouping, reducing some value/function.
Commonly used operations on paired RDD are: groupByKey() reduceByKey() countByKey() join() etc
Creation of Paired RDD:
val pRDD:[(String),(Int)]=sc.textFile(“path_of_your_file”)
.flatMap(line => line.split(” “))
.map{word=>(word,word.length)}



Also using subString method(if we have a file with id and some value, we can create paired rdd with id as key and value as other details)

val pRDD2[(Int),(String)]=sc.textFile(“path_of_your_file”)
.keyBy(line=>line.subString(1,5).trim().toInt)
.mapValues(line=>line.subString(10,30).trim())"
---------------------------------------------------------------------------------------------------------------------------------------------------	

66	"How is RDD in Spark different from Distributed Storage Management?


RDDs store data in-memory (unless explicitly cached).
Distributed Storage stores data in persistent storage.

RDDs can re-compute itself in the case of failure or data loss.
If data is lost from the Distributed Storage system it is gone forever"
---------------------------------------------------------------------------------------------------------------------------------------------------	

67	"Explain transformation and action in RDD in Apache Spark.


Transformations are operations on RDD that create one or more new RDDs. E.g. map, filter, reduceByKey etc. In other words, transformations are functions that take an RDD as the input and produce one or more RDDs as the output. There is no change in the input RDD, but it always produces one or more new RDDs by applying the computations they represent.Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed.

Actions are RDD operations that produce non-RDD values. In other words, an RDD operation that returns a value of any type but an RDD is an action. They trigger execution of RDD transformations to return values. Simply put, an action evaluates the RDD lineage graph. E.g. collect, reduce, count, foreach etc."
68	"What are the types of Apache Spark transformation?

there are fundamentally two types of transformations:

1. Narrow transformation –
While talking about Narrow transformation, all the elements which are required to compute the records in single partition reside in the single partition of parent RDD. To calculate the result, a limited subset of partition is used. This Transformation are the result of map(), filter().

2. Wide Transformations – 
Wide transformation means all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. Partitions may reside in many different partitions of parent RDD. This Transformation is a result of groupbyKey() and reducebyKey()."
69	"Explain the RDD properties.


1. List or Set of partitions.
2. List of dependencies on other (parent) RDD
3. A function to compute each partition
Below operations are used for optimization during execution.

4. Optional preferred location [i.e. block location of an HDFS file] [it’s about data locality] 
5. Optional partitioned info [i.e. Hash-Partition for Key/Value pair –> When data shuffled how data will be traveled]"
70	"What is lineage graph in Apache Spark?


When we apply a different transformation on RDD it creates RDD Linage graph. It is a new RDD from already existing RDDs. It is the dependencies graph between the existing and the new RDD formed. the need of RDD lineage graph arrives when we want to compute new RDD or if we want to recover the lost data from the lost persisted RDD.


You can check lineage between two RDDs using rdd0.toDebugString. This gives back you the lineage graph from current rdd to all the previous dependencies of RDDs. See below. Whenever you see “+-” symbol from the toDebugString output, it means there will be next stage from the next operation onwards. This is indicates to identify that how many stage are created."
71	" Explain the terms  Spark Partitions and Partitioners.

Ans. Partition in Spark is similar to split in HDFS. A partition in Spark is a logical division of data stored on a node in the cluster. They are the basic units of parallelism in Apache Spark. RDDs are a collection of partitions.When some actions are executed, a task is launched per partition.

By default, partitions are automatically created by the framework. However, the number of partitions in Spark are configurable to suit the needs.
For the number of partitions, if spark.default.parallelism is set, then we should use the value from SparkContext defaultParallelism, othewrwise we suould use the max number of upstream partitions. Unless spark.default.parallelism is set, the number of partitions will be the same as that of the largest upstream RDD, as this would least likely cause an out-of-memory errors.

A partitioner is an object that defines how the elements in a key-value pair RDD are partitioned by key, maps each key to a partition ID from 0 to numPartitions – 1. It captures the data distribution at the output. With the help of partitioner, the scheduler can optimize the future operations. The contract of partitioner ensures that records for a given key have to reside on a single partition.
We should choose a partitioner to use for a cogroup-like operations. If any of the RDDs already has a partitioner, we should choose that one. Otherwise, we use a default HashPartitioner.

There are three types of partitioners in Spark :
a) Hash Partitioner b) Range Partitioner c) Custom Partitioner
Hash – Partitioner : Hash- partitioning attempts to spread the data evenly across various partitions based on the key.
Range – Partitioner : In Range- Partitioning method , tuples having keys with same range will appear on the same machine.

RDDs can be created with specific partitioning in two ways :
i) Providing explicit partitioner by calling partitionBy method on an RDD
ii) Applying transformations that return RDDs with specific partitioners"
72	" By Default, how many partitions are created in RDD in Apache Spark?


By Default, Spark creates one Partition for each block of the file (For HDFS)

val rdd1 = sc.textFile(""/home/hdadmin/wc-data.txt"")

val rdd1 = sc.textFile(""/home/hdadmin/wc-data.txt"", 10) --this will create 10 partition 

//To check no. of partition 

rdd1.partitions.length or  rdd1.getNumPartitions"
73	"What is Spark DataFrames?


DataFrame consists of two words data and frame, means data has to be fit in some kind of frame. We can understand a frame as a schema of the relational database.

In Spark, DataFrame is a collection of distributed data over the network with some schema. We can understand it as the data formatted as row/column manner. DataFrame can be created from Hive data, JSON file, CSV, Structured data or raw data that can be framed in structured data. We can also create a DataFrame from RDD if some schema can be applied on that RDD.
Temporary view or table can also be created from DataFrame as it has data and schema. We can also run SQL query on created table/view to get the faster result.
It is also evaluated lazily (Lazy Evaluation) for better resource utilization."
74	"What are benefits of DataFrame in Spark?


In DataFrames, data is organized in named column.
DataFrames empower SQL queries and the DataFrame API.
we can process both structured and unstructured data formats through it. Such as: Avro, CSV, elastic search, and Cassandra. Also, it deals with storage systems HDFS, HIVE tables, MySQL, etc.
In DataFrames, Catalyst supports optimization(catalyst Optimizer).
DataFrame provides easy integration with Big data tools and framework via Spark core."
75	"What is Spark Dataset?

A Dataset is an immutable collection of objects, those are mapped to a relational schema. They are strongly-typed in nature.There is an encoder, at the core of the Dataset API. That Encoder is responsible for converting between JVM objects and tabular representation. By using Spark’s internal binary format, the tabular representation is stored that allows to carry out operations on serialized data and improves memory utilization.


Datasets are ""lazy"", i.e. computations are only triggered when an action is invoked. Internally, a Dataset represents a logical plan that describes the computation required to produce the data. When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a physical plan for efficient execution in a parallel and distributed manner. To explore the logical plan as well as optimized physical plan, use the explain.function.
 val people = spark.read.parquet(""..."")
   val department = spark.read.parquet(""..."")

   people.filter(""age > 30"")
     .join(department, people(""deptId"") === department(""id""))
     .groupBy(department(""name""), people(""gender""))
     .agg(avg(people(""salary"")), max(people(""age"")))
To efficiently support domain-specific objects, an Encoder is required. The encoder maps the domain specific type T to Spark's internal type system. For example, given a class Person with two fields, name (string) and age (int), an encoder is used to tell Spark to generate code at runtime to serialize the Person object into a binary structure. This binary structure often has much lower memory footprint as well as are optimized for efficiency in data processing (e.g. in a columnar format). To understand the internal binary representation for data, use the schema function."
76	"What are the advantages of datasets in spark?


Static typing feature of Dataset, a developer can catch errors at compile time 

Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan providing the space and speed efficiency."
77	"What is Directed Acyclic Graph in Apache Spark?


Directed Acyclic Graph is a graph with cycles which are not directed. DAG is a graph which contains set of all the operations that are applied on RDD. On RDD when any action is called. Spark creates the DAG and submits it to the DAG scheduler. Only after the DAG is built, Spark creates the query optimization plan. The DAG scheduler divides operators into stages of tasks. A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together.
Fault tolerance is achieved in Spark using the Directed Acyclic Graph. The query optimization is possible in Spark by the use of DAG."
78	"What is the difference between DAG and Lineage?


Lineage graph
As we know, that whenever a series of transformations are performed on an RDD, they are not evaluated immediately, but lazily(Lazy Evaluation). When a new RDD has been created from an existing RDD, that new RDD contains a pointer to the parent RDD. Similarly, all the dependencies between the RDDs will be logged in a graph, rather than the actual data. This graph is called the lineage graph.

Now coming to DAG,

Directed Acyclic Graph(DAG)
DAG in Apache Spark is a combination of Vertices as well as Edges. In DAG vertices represent the RDDs and the edges represent the Operation to be applied on RDD. Every edge in DAG is directed from earlier to later in a sequence.When we call anAction, the created DAG is submitted to DAG Scheduler which further splits the graph into the stages of the task."
79	" What is the difference between Caching and Persistence in Apache Spark?



Cache is a synonym of Persist with MEMORY_ONLY storage level(i.e) using Cache technique we can save intermediate results in memory only when needed.

Persist marks an RDD for persistence using storage level which can be MEMORY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2"
80	"What are the limitations of Apache Spark?


1. No File Management System
Apache Spark relies on other platforms like Hadoop or some another cloud-based Platform for file management system. This is one of the major issues with Apache Spark.


3. No support for Real-Time Processing
In Spark Streaming, the arriving live stream of data is divided into batches of the pre-defined interval, and each batch of data is treated like Spark Resilient Distributed Database (RDDs). Then these RDDs are processed using the operations like map, reduce, join etc. The result of these operations is returned in batches. Thus, it is not real-time processing but Spark is near real-time processing of live data. Micro-batch processing takes place in Spark Streaming.


4. Manual Optimization
Manual Optimization is required to optimize Spark jobs. Also, it is adequate to specific datasets. we need to control manually if we want to partition and cache in Spark to be correct.


6. Window Criteria
Spark does not support record based window criteria. It only has time-based window criteria.


8. Expensive
when we want cost-efficient processing of big data In-memory capability can become a bottleneck as keeping data in memory is quite expensive. At that time the memory consumption is very high, and it is not handled in a user-friendly manner. The cost of Spark is quite high because Apache Spark requires lots of RAM to run in-memory."
81	"Different Running Modes of Apache Spark


We can launch spark application in four modes:

1) Local Mode (local[*],local,local[2]…etc)
-> When you launch spark-shell without control/configuration argument, It will launch in local mode
spark-shell –master local[1]
-> spark-submit –class com.df.SparkWordCount SparkWC.jar local[1]

2) Spark Standalone cluster manger:
-> spark-shell –master spark://hduser:7077
-> spark-submit –class com.df.SparkWordCount SparkWC.jar spark://hduser:7077

3) Yarn mode (Client/Cluster mode):
-> spark-shell –master yarn or
(or)
->spark-shell –master yarn –deploy-mode client

Above both commands are same.
To launch spark application in cluster mode, we have to use spark-submit command. We cannot run yarn-cluster mode via spark-shell because when we run spark application, driver program will be running as part application master container/process. So it is not possible to run cluster mode via spark-shell.
-> spark-submit –class com.df.SparkWordCount SparkWC.jar yarn-client
-> spark-submit –class com.df.SparkWordCount SparkWC.jar yarn-cluster

4) Mesos mode:
-> spark-shell –master mesos://HOST:5050"
82	" What are the different ways of representing data in Spark?

Different Ways of representing data in Spark are:-

RDD-:Spark revolves around the concept of a resilient distributed dataset (RDD),
which is a fault-tolerant collection of elements that can be operated on in parallel.
There are two ways to create RDDs:
1) parallelizing an existing collection in your driver program
2) referencing a dataset in an external storage system,
such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.
3)Existing RDDs – Creating RDD from already existing RDDs.
By applying transformation operation on existing RDDs we can create new RDD.

DataFrame:-DataFrame is an abstraction which gives a schema view of data.
Which means it gives us a view of data as columns with column name and types info,
We can think data in data frame like a table in database.
-Like RDD, execution in Dataframe too is lazy triggered .-offers huge performance
improvement over RDDs because of 2 powerful features it has:
1. Custom Memory management :Data is stored in off-heap memory in binary format.
This saves a lot of memory space. Also there is no Garbage Collection overhead involved.
By knowing the schema of data in advance and storing efficiently in binary format,
expensive java Serialization is also avoided.
2. Optimized Execution Plans :Query plans are created for execution using Spark catalyst
optimiser. After an optimised execution plan is prepared going through some steps,
the final execution happens internally on RDDs only but thats completely hidden from the
users.

DataSet:-Datasets in Apache Spark are an extension of DataFrame API which provides
type-safe, object-oriented programming interface.
Dataset takes advantage of Spark’s Catalyst optimizer by exposing expressions and
data fields to a query planner.

Dataset and DataFrame internally does final execution on RDD objects only but the difference
is users do not write code to create the RDD collections and have no control as such over RDDs."
83	"What is write ahead log(journaling) in Spark?

There are two types of failures in any Apache Spark job – Either the driver failure or the worker failure.

When any worker node fails, the executor processes running in that worker node will be killed, and the tasks which were scheduled on that worker node will be automatically moved to any of the other running worker nodes, and the tasks will be accomplished.

When the driver or master node fails, all of the associated worker nodes running the executors will be killed, along with the data in each of the executors’ memory. In the case of files being read from reliable and fault tolerant file systems like HDFS, zero data loss is always guaranteed, as the data is ready to be read anytime from the file system. Checkpointing also ensures fault tolerance in Spark by periodically saving the application data in specific intervals.

In the case of Spark Streaming application, zero data loss is not always guaranteed, as the data will be buffered in the executors’ memory until they get processed. If the driver fails, all of the executors will be killed, with the data in their memory, and the data cannot be recovered.

To overcome this data loss scenario, Write Ahead Logging (WAL) has been introduced in Apache Spark 1.2. With WAL enabled, the intention of the operation is first noted down in a log file, such that if the driver fails and is restarted, the noted operations in that log file can be applied to the data. For sources that read streaming data, like Kafka or Flume, receivers will be receiving the data, and those will be stored in the executor’s memory. With WAL enabled, these received data will also be stored in the log files.

WAL can be enabled by performing the below:

1. Setting the checkpoint directory, by using streamingContext.checkpoint(path)

2. Enabling the WAL logging, by setting spark.stream.receiver.WriteAheadLog.enable to True."
84	"Explain catalyst query optimizer in Apache Spark.


“The term optimization refers to a process in which a system is modified in such a way that it work more efficiently or it uses fewer resources.”
Spark SQL is the most technically involved component of Apache Spark. Spark SQL deals with both SQL queries and DataFrame API. In the depth of Spark SQL there lies a catalyst optimizer. Catalyst optimization allows some advanced programming language features that allow you to build an extensible query optimizer.
A new extensible optimizer called Catalyst emerged to implement Spark SQL. This optimizer is based on functional programming construct in Scala.
Catalyst Optimizer supports both rule-based and cost-based optimization. In rule-based optimization the rule based optimizer use set of rule to determine how to execute the query. While the cost based optimization finds the most suitable way to carry out SQL statement. In cost-based optimization, multiple plans are generated using rules and then their cost is computed.


Catalyst optimizer makes use of standard features of Scala programming like pattern matching. In the depth, Catalyst contains the tree and the set of rules to manipulate the tree. There are specific libraries to process relational queries. There are various rule sets which handle different phases of query execution like analysis, query optimization, physical planning, and code generation to compile parts of queries to Java bytecode."
85	"What are shared variables in Apache Spark?


Shared variables are nothing but the variables that can be used in parallel operations. By default, when Apache Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums."
86	"How does Apache Spark handles accumulated Metadata?


Metadata accumulates on the driver as consequence of shuffle operations. It becomes particularly tedious during long-running jobs.
To deal with the issue of accumulating metadata, there are two options:

First, set the spark.cleaner.ttl parameter to trigger automatic cleanups. However, this will vanish any persisted RDDs.
The other solution is to simply split long-running jobs into batches and write intermediate results to disk. This facilitates a fresh environment for every batch and don’t have to worry about metadata build-up."
87	"what is breeze?

 Breeze is a collection of libraries for numerical computing and machine learning."
88	"What is Apache Spark Machine learning library?


To make machine learning scalable and easy, MLlib is created. There are machine learning libraries that included an implementation of various machine learning algorithms. For example, clustering, regression, classification and collaborative filtering. Some lower level machine learning primitives like generic gradient descent optimization algorithm are also present in MLlib.


MLlib is switching to DataFrame API. Some of the benefits of using DataFrames are it includes Spark Data sources, Spark SQL DataFrame queries Tungsten and Catalyst optimizations, and uniform APIs across languages."
89	"List commonly used Machine Learning Algorithm.



(1) Supervised Learning Algorithm
(2) Unsupervised Learning Algorithm
(3) Reinforcement Learning Algorithm

> Most commonly used Machine Learning Algorithm are as follows :
(1) Linear Regression
(2) Logistic Regression
(3) Decision Tree
(4) K-Means
(5) KNN
(6) SVM
(7) Random Forest
(8) Naïve Bayes
(9) Dimensionality Reduction Algorithm
(10) Gradient Boost and Adaboost"
90	"What is the difference between DSM and RDD?


On the basis of several features, the difference between RDD and DSM is:

i. Read

RDD – The read operation in RDD is either coarse-grained or fine-grained. Coarse-grained meaning we can transform the whole dataset but not an individual element on the dataset. While fine-grained means we can transform individual element on the dataset.
DSM – The read operation in Distributed shared memory is fine-grained.

ii. Write

RDD – The write operation in RDD is coarse-grained.
DSM – The Write operation is fine grained in distributed shared system.


Fault-Recovery Mechanism

RDD – By using lineage graph at any moment, the lost data can be easily recovered in Spark RDD. Therefore, for each transformation, new RDD is formed. As RDDs are immutable in nature, hence, it is easy to recover.
DSM – Fault tolerance is achieved by a checkpointing technique which allows applications to roll back to a recent checkpoint rather than restarting.


Straggler Mitigation

Stragglers, in general, are those that take more time to complete than their peers. This could happen due to many reasons such as load imbalance, I/O blocks, garbage collections, etc.
An issue with the stragglers is that when the parallel computation is followed by synchronizations such as reductions that causes all the parallel tasks to wait for others.

RDD – It is possible to mitigate stragglers by using backup task, in RDDs.
DSM – To achieve straggler mitigation, is quite difficult.

vi. Behavior if not enough RAM

RDD – As there is not enough space to store RDD in RAM, therefore, the RDDs are shifted to disk.
DSM – If the RAM runs out of storage, the performance decreases, in this type of systems."
91	"What is lazy evaluation in Spark?


Lazy evaluation means the execution will not start until anaction is triggered. Transformations are lazy in nature i.e. when we call some operation on RDD, it does not execute immediately. Spark adds them to a DAG of computation and only when driver requests some data, this DAG actually gets executed

Advantages of lazy evaluation.

1) It is an optimization technique i.e. it provides optimization by reducing the number of queries.
2) It saves the round trips between driver and cluster, thus speeds up the process.
Spark uses lazy evaluation to reduce the number of passes it has to take over our data by grouping operations together. In case MapReduce, user/developer has to spend a lot of time on how to group operations together in order to minimize the number of MapReduce passes. In spark, there is no benefit of writing a single complex map instead of chaining together many simple operations. The user can organize their spark program into smaller operations. Spark will be managed very efficiently of all the operations by using lazy evaluation"
92	"What are the ways to launch Apache Spark over YARN?


Apache Spark has two modes of running applications on YARN: cluster and client
spark-submit or spark-shell –master yarn-cluster or –master yarn-client"
93	"What is Speculative Execution in Apache Spark?


The Speculative task in Apache Spark is task that runs slower than the rest of the task in the job.It is health check process that verifies the task is speculated, meaning the task that runs slower than the median of successfully completed task in the task sheet. Such tasks are submitted to another worker. It runs the new copy in parallel rather than shutting down the slow task.


Spark Property >> Default Value >> Description
spark.speculation >> false >> enables ( true ) or disables ( false ) speculative execution of tasks.
spark.speculation.interval >> 100ms >> The time interval to use before checking for speculative tasks.
spark.speculation.multiplier >> 1.5 >> How many times slower a task is than the median to be for speculation.
spark.speculation.quantile >> 0.75 >> The percentage of tasks that has not finished yet at which to start speculation."
94	"How can data transfer be minimized when working with Apache Spark?


In Spark, Data Transfer can be reduced by avoiding operation which results in data shuffle.
Avoid operations like repartition and coalesce, ByKey operations like groupByKey and reduceByKey, and join operations like cogroup and join.

Spark Shared Variables help in reducing data transfer. There two types for shared variables-Broadcast variable and Accumulator.

Broadcast variable:

If we have a large dataset, instead of transferring a copy of data set for each task, we can use a broadcast variable which can be copied to each node at one time
and share the same data for each task in that node. Broadcast variable help to give a large data set to each node.
First, we need to create a broadcast variable using SparkContext.broadcast and then broadcast the same to all nodes from driver program. Value method
can be used to access the shared value. The broadcast variable will be used only if tasks for multiple stages use the same data.

Accumulator:

Spark functions used variables defined in the driver program and local copied of variables will be generated. Accumulator are shared variables which help to update
variables in parallel during execution and share the results from workers to the driver."
95	"What is action, how it process data in apache spark


Actions return final result of RDD computations/operation.It triggers execution using lineage graph to load the data into original RDD, and carries out all intermediate transformations and returns final result to Driver program or write it out to file system.

For example: First, take, reduce, collect, count, aggregate,min,max,sum are some of the actions in spark."
96	"How is fault tolerance achieved in Apache Spark?



The basic semantics of fault tolerance in Apache Spark is, all the Spark RDDs are immutable. It remembers the dependencies between every RDD involved in the operations, through the lineage graph created in the DAG, and in the event of any failure, Spark refers to the lineage graph to apply the same operations to perform the tasks.


There are two types of failures – Worker or driver failure. In case if the worker fails, the executors in that worker node will be killed, along with the data in their memory. Using the lineage graph, those tasks will be accomplished in any other worker nodes. The data is also replicated to other worker nodes to achieve fault tolerance. There are two cases:

1.Data received and replicated – Data is received from the source, and replicated across worker nodes. In the case of any failure, the data replication will help achieve fault tolerance.

2.Data received but not yet replicated – Data is received from the source but buffered for replication. In the case of any failure, the data needs to be retrieved from the source.

For stream inputs based on receivers, the fault tolerance is based on the type of receiver:

<li style=”list-style-type: none”>
Reliable receiver – Once the data is received and replicated, an acknowledgment is sent to the source. In case if the receiver fails, the source will not receive acknowledgment for the received data. When the receiver is restarted, the source will resend the data to achieve fault tolerance.
Unreliable receiver – The received data will not be acknowledged to the source. In this case of any failure, the source will not know if the data has been received or not, and it will nor resend the data, so there is data loss.
To overcome this data loss scenario, Write Ahead Logging (WAL) has been introduced in Apache Spark 1.2. With WAL enabled, the intention of the operation is first noted down in a log file, such that if the driver fails and is restarted, the noted operations in that log file can be applied to the data. For sources that read streaming data, like Kafka or Flume, receivers will be receiving the data, and those will be stored in the executor’s memory. With WAL enabled, these received data will also be stored in the log files.

WAL can be enabled by performing the below:

Setting the checkpoint directory, by using streamingContext.checkpoint(path)
Enabling the WAL logging, by setting spark.stream.receiver.WriteAheadLog.enable to True."
97	"What is the role of Spark Driver in spark applications?


n easy terms, the driver in Spark creates SparkContext, connected to a given Spark Master.It conjointly delivers the RDD graphs to Master, wherever the standalone cluster manager runs.
It splits a Spark application into tasks and schedules them to run on executors.
A driver is where the task scheduler lives and spawns tasks across workers.
A driver coordinates workers and overall execution of tasks.
Whenever a client runs the application code, the driver programs instantiates Spark Context, converts the transformations and actions into logical DAG of execution. This logical DAG is then converted into a physical execution plan, which is then broken down into smaller physical execution units. The driver then interacts with the cluster manager to negotiate the resources required to perform the tasks of the application code. The cluster manager then interacts with each of the worker nodes to understand the number of executors running in each of them."
98	"Explain Accumulator in Spark.

Accumulator is a shared variable in Apache Spark, used to aggregating information across the cluster.In other words, aggregating information / values from worker nodes back to the driver program. 


Why Accumulator :


When we use a function inside the operation like map(), filter() etc these functions can use the variables which defined outside these function scope in the driver program.When we submit the task to cluster, each task running on the cluster gets a new copy of these variables and updates from these variable do not propagated back to the driver program.Accumulator lowers this restriction.


Use Case :

Meaning count the no. of blank lines from the input file, no. of bad packets from network during session, during Olympic data analysis we have to find age where we said (age != ‘NA’) in SQL query in short finding bad / corrupted records.

Example :

scala> val record = spark.read.textFile(""/home/hdadmin/wc-data-blanklines.txt"")

val emptylines = sc.accumulator(0)

val processdata = record.flatMap(x =>
					        {
							if(x == """")
								 emptylines += 1
							 x.split("" "")
						 })


scala> processdata.collect           //will show non-empty lines 

scala> println(“No. of Empty Lines : ” + emptylines.value)  //show empty lines 

Please note that, task(s) on worker nodes can not access the value property of accumulator so for in context of task(s), accumulator is write-only variable.


The value() property of accumulator is available only in the driver program.


We can also count the no. of blank lines with the help of transformation/actions but for that, we need an extra operation but with the help of accumulator, we can count the no. of blank lines (or events in broader terms) as we load /process our data.
//Imp point

Please keep in mind that, for accumulators which are used in action operation, Spark applies each task’s update only once. Hence we want to reliable result from accumulator, in that case, we must use it inside actions regardless of failure of multiple evaluations .If we use the accumulator in transformation, in that case, this guarantee does not exist. An accumulator update within transformation can occur more than once."
99	"What is the role of Driver program in Spark Application?

Driver program is responsible for launching various parallel operations on the cluster.
Driver program contains application’s main() function.
Driver program access Apache Spark through a SparkContext object which represents a connection to computing cluster (From Spark 2.0 onwards we can access SparkContext object through SparkSession).
Driver program is responsible for converting user program into the unit of physical execution called task.
Spark program creates a logical plan called Directed Acyclic graph which is converted to physical execution plan by the driver when driver program runs."
100	"How to identify that given operation is Transformation/Action in your program?


In order to identify the operation, one need to look at the return type of an operation.
If operation returns a new RDD in that case an operation is ‘Transformation’
If operation returns any other type than RDD in that case an operation is ‘Action’"
