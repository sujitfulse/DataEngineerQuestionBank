1	"why we need to have such a huge blocks size i.e. 128 MB?
  Well, whenever we talk about HDFS, we talk about huge data sets, i.e. Terabytes and Petabytes of data. So, if we had a block size of let’s say of 4 KB, as in Linux file system, we would be having too many blocks and therefore too much of the metadata. So, managing these no. of blocks and metadata will create huge overhead, which is something, we don’t want."

2	What is Your Cluster size and Data Handling Size
3	How to Transfer data from One Cluster to Another cluster
4	What are the Optimization techniques used in Hive
5	Explain Bucketing and Partitioning In which scenarios you use both in your Project
6	Write the Query to Create Bucketing table
7	Clustered By Distributed By Difference
8	who submits the job name node or data node
9	who executes the job name node or data node
10	How do you fix production issue and tell me few issues you fixed in your project
11	what are cores 
