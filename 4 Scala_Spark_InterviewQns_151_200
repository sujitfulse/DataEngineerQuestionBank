151	"write a program where we can filter all null values and concat an '_' with all non null values

val newDf =   df.withColumn("NEW_COLUMN",concat(when(col(""COL1"").isNotNull, col(""COL1"")),'-'));
      
 -------------------------------------------------------------------------------------------------------------------------------------------------------  
 
152	What are the roles and responsibilities of worker nodes in the Apache Spark cluster? Is Worker Node in Spark is same as Slave Node?

      Apache Spark follows a master/slave architecture, with one master or driver process and more than one slave or worker processes
      
      1. The master is the driver that runs the main() program where the spark context is created. 
         It then interacts with the cluster manager to schedule the job execution and perform the tasks.

      2. The worker consists of processes that can run in parallel to perform the tasks scheduled by the driver program. 
         These processes are called executors.

      Whenever a client runs the application code, the driver programs instantiates Spark Context, converts the transformations and 
      actions into logical DAG of execution. 
      This logical DAG is then converted into a physical execution plan, which is then broken down into smaller physical execution units. 
      The driver then interacts with the cluster manager to negotiate the resources required to perform the tasks of the application code. 
      The cluster manager then interacts with each of the worker nodes to understand the number of executors running in each of them.

      The role of worker nodes/executors:

      1. Perform the data processing for the application code
      2. Read from and write the data to the external sources
      3. Store the computation results in memory, or disk.
     
      Before the execution of tasks, the executors are registered with the driver program through the cluster manager, 
      so that the driver knows how many numbers of executors are running to perform the scheduled tasks. 
      The executors then start executing the tasks scheduled by the worker nodes through the cluster manager.

      Whenever any of the worker nodes fail, the tasks that are required to be performed will be automatically allocated to any other worker nodes"
------------------------------------------------------------------------------------------------------------------------------------------------------- 

153	How to chain Dataframe transformation in Spark Transform method ?
    
  implicit classes or the Dataset#transform method can be used to chain DataFrame transformations in Spark.
  Though Dataset#transform method is preferred compared to implicit classes.

      Eg:

      val df = Seq("funny","person").toDF("something")

      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }

      def withFarewell(df: DataFrame): DataFrame = {
        df.withColumn(""farewell"", lit(""goodbye""))
      }


      val weirdDf = df
        .transform(withGreeting)
        .transform(withFarewell)
    
       weirdDf.show()
      +---------+-----------+--------+
      |something|   greeting|farewell|
      +---------+-----------+--------+
      |    funny|hello world| goodbye|
      |   person|hello world| goodbye|

      Transform Method with Arguments
-----------------------------
      def withGreeting(df: DataFrame): DataFrame = {
        df.withColumn(""greeting"", lit(""hello world""))
      }
      def withCat(name: String)(df: DataFrame): DataFrame = {
        df.withColumn(""cats"", lit(s""$name meow""))
      }

      val niceDf = df
        .transform(withGreeting)
        .transform(withCat(""puffy""))
---------------------------

      Monkey Patching with Implicit Classes :
      Implicit classes can be used to add methods to existing classes. 
      The following code adds the same withGreeting() and withFarewell() methods to the DataFrame class itself.

      object BadImplicit {
        implicit class DataFrameTransforms(df: DataFrame) {
          def withGreeting(): DataFrame = {
            df.withColumn(""greeting"", lit(""hello world""))
          }
          def withFarewell(): DataFrame = {
            df.withColumn(""farewell"", lit(""goodbye""))
          }
        }
      }
      The withGreeting() and withFarewell() methods can be chained and executed as follows.

      import BadImplicit._
      val df = Seq("funny","person").toDF(""something"")
      val hiDf = df.withGreeting().withFarewell()"
------------------------------------------------------------------------------------------------------------------------------------------------------- 
154	difference between traits and abstract class ?

      Traits
      Traits are similar to interfaces in Java and are created using trait keyword.

      Abstract Class
      Abstract Class is similar to abstract classes in Java and are created using abstract keyword.

      1) Multiple inheritance : 	
      Trait supports multiple inheritance.	
      Abstract Class supports single inheritance only.
      2) Instance	
      Trait can be added to an object instance.	
      Abstract class cannot be added to an object instance.
      3) Constructor parameters	
      Trait cannot have parameters in its constructors.	
      Abstract class can have parameterised constructor.
      4) Interoperability	
      Traits are interoperable with java if they don't have any implementation.	
      Abstract classes are interoperable with java without any restriction.
      5) Stackability	
      Traits are stackable and are dynamically bound.	
      Abstract classes are not stacable and are statically bound.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

163	difference between map and foreach ? when to use a map and when to use foreach ?

      map :
      the map function in the Seq trait returns a value.
      It is designed to apply a function to every element of a collection extending the Seq trait and return a new collection.
      def map[B](f: (A) â‡’ B): Seq[B]

      foreach, in the same trait, has the following signature:
      def foreach(f: (A) â‡’ Unit): Unit

      val l: List[String] = List("a", "b", "c")
      l.map(e => e.toUpperCase(java.util.Locale.ROOT))
      l.foreach(e => println(e))
------------------------------------------------------------------------------------------------------------------------------------------------------- 

164	what is case class and when we will use a case class ?

    Scala case classes are just regular classes which are immutable by default and decomposable through pattern matching.
    It uses equal method to compare instance structurally. It does not use new keyword to instantiate object.
    All the parameters listed in the case class are public and immutable by default.
    
    case class className(parameters)  
    
    case class CaseClass(a:Int, b:Int)    
    object MainObject{  
        def main(args:Array[String]){  
            var c =  CaseClass(10,10)       // Creating object of case class  
            println("a = "+c.a)               // Accessing elements of case class  
            println("b = "+c.b)  
        }  
    }  

A case class which has no arguments is declared as case object instead of case class. case object is serializeable by default.

      trait SuperTrait  
      case class CaseClass1(a:Int,b:Int) extends SuperTrait  
      case class CaseClass2(a:Int) extends SuperTrait         // Case class  
      case object CaseObject extends SuperTrait               // Case object  
      object MainObject{  
          def main(args:Array[String]){  
              callCase(CaseClass1(10,10))  
              callCase(CaseClass2(10))  
              callCase(CaseObject)  
          }  
          def callCase(f:SuperTrait) = f match{  
              case CaseClass1(f,g)=>println("a = "+f+" b ="+g)  
              case CaseClass2(f)=>println("a = "+f)  
              case CaseObject=>println("No Argument")  
          }  
      }  
------------------------------------------------------------------------------------------------------------------------------------------------------- 

165	what if partion is deleted from external table and will it give error while a select ?

    - To delete data in external table you need to delete files on the filesystem. Data in the external table will remain if you drop table or partition. 
------------------------------------------------------------------------------------------------------------------------------------------------------- 
  
166 MSCK REPAIR TABLE command
    -  MSCK REPAIR TABLE command. This will restore all the partitions that are not in the metastore but exist on the file system, 
    So, with an external table, if you want the data in a partition to be gone when you drop it, 
    you have to also go and delete it from the filesystem. Otherwise, the data will continue to exist and 
    there is a good chance that someone will bring the metadata for it back into the metastore.
    
   * convert external table into managed one- 
    ALTER TABLE abc SET TBLPROPERTIES('EXTERNAL'='FALSE');
    
   * DELETE DEFAULT PARTITIONS FROM HIVE.
    ALTER TABLE Table_Name DROP IF EXISTS PARTITION(process_date='__HIVE_DEFAULT_PARTITION__')

------------------------------------------------------------------------------------------------------------------------------------------------------- 

167	how to improve performance of two big tables in hive ?
  optimize.bucketmapjoin=true;
------------------------------------------------------------------------------------------------------------------------------------------------------- 

169	spark scala word count programm

RDD : 
 val text = sc.textFile("mytextfile.txt") 
 val counts = text.flatMap(line => line.split(" ") ).map(word => (word,1)).reduceByKey(_+_) counts.collect 

DataFrame :
val df = sqlContext.read.text("README.md")
val wordsDF = df.select(split(df("value")," ").alias("words"))
val wordDF = wordsDF.select(explode(wordsDF("words")).alias("word"))
val wordCountDF = wordDF.groupBy("word").count

------------------------------------------------------------------------------------------------------------------------------------------------------- 
170	What is future class in scala
  Futures allows performing many operations in parallelâ€“ in an efficient and non-blocking way.
  A Future is a placeholder object for a value that may not yet exist. 
  Generally, the value of the Future is supplied concurrently and can subsequently be used. 
  Composing concurrent tasks in this way tends to result in faster, asynchronous, non-blocking parallel code.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

176	what internally happens when you do spark-submit ?

https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2019/11/Internals-of-Job-Execution-In-Spark.jpg
------------------------------------------------------------------------------------------------------------------------------------------------------- 
177	 How to read binary files in spark ?

  assuming you have no delimiter, 
  an Int in Scala is 4 bytes,
  Short is 2 byte, 
  long is 8 bytes. 
  You should be able to take the bytes and convert them to the classes you want.

   import java.nio.ByteBuffer
  val result = YourRDD.map(x=>(ByteBuffer.wrap(x.take(4)).getInt,
             ByteBuffer.wrap(x.drop(4).take(2)).getShort,
             ByteBuffer.wrap(x.drop(6)).getLong))
------------------------------------------------------------------------------------------------------------------------------------------------------- 

178	List hive  analytic functions you used in project 
==> lead , lag, Rownumber
------------------------------------------------------------------------------------------------------------------------------------------------------- 

179	how will you check if data is there or not in 6th partition in RDD ? 
==> using glom() methond
  val rdd = sc.parallelize(Seq(1,2,3,4,5,6,7,8,9,10))
  rdd.glom().collect()

  //Result
  res3: Array[Array[Int]] = Array(Array(1), Array(2), Array(3), Array(4, 5), Array(6), Array(7), Array(8), Array(9, 10))
------------------------------------------------------------------------------------------------------------------------------------------------------- 
183 what is stage n task in spark

Spark Stages : 
Whenever there is a shuffling of data over the network, Spark divides the job into multiple stages. 
Therefore, a stage is created when the shuffling of data takes place.
These stages can be either processed parallelly or sequentially depending upon the dependencies of these stages between each other. 

There are two types of stages in Spark:

1.ShuffleMapStage in Spark
2. ResultStage in Spark
------------------------------------------------------------------------------------------------------------------------------------------------------- 
184	in case one task is taking more time how will you handle --> Specutive execution.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

85	Rank vs DenseRank
Rank - same rank to mulple records if tie, but then skips sequence. ( 1,1,3,4)
DenseRank - same rank to mulple records if tie, but then skips sequence.. dense_rank leaves no gaps in ranking sequence when there are ties.( 1,1,2,3)
------------------------------------------------------------------------------------------------------------------------------------------------------- 

186	InferSchema method API ? 

- By setting inferSchema=true, Spark will automatically go through the csv file and infer the schema of each column.
This requires an extra pass over the file which will result in reading a file with inferSchema set to true being slower. 
But in return the dataframe will most likely have a correct schema given its input.
------------------------------------------------------------------------------------------------------------------------------------------------------- 
187,188	Map Side Join
-> 
 if you need to join a large table (fact) with relatively small tables (dimensions) i.e. to perform a star-schema join you can avoid sending all data 
 of the large table over the network. This type of join is called map-side join in Hadoop community.
 In other distributed systems, it is often called replicated or broadcast join.
------------------------------------------------------------------------------------------------------------------------------------------------------- 

189	SMB joins

Shuffle phase
Data from both datasets are read and shuffled. After the shuffle operation, records with the same keys from both datasets will end up in the same partition 
after the shuffle. Note that the entire dataset is not broadcasted with this join. This means the dataset in each partition will be in a manageable size 
after the shuffle.

Sort phase 
Records on both sides are sorted by key. Hashing and bucketing are not involved with this join.

Merge phase
A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted the merge or the join operation is stopped for an element 
as soon as a key mismatch is encountered. So a join attempt is not performed on all keys.

------------------------------------------------------------------------------------------------------------------------------------------------------- 

195	how we do GC in Spark
JVM garbage collection is problematic with large churn RDD stored by the program. To make room for new objects, Java removes the older one; it traces all the
old objects and finds the unused one. But the key point is that cost of garbage collection in Spark is proportional to a number of Java objects. Thus, it is
better to use a data structure in Spark with lesser objects. 
------------------------------------------------------------------------------------------------------------------------------------------------------- 
196	have you ever received an error "no-space-left-on-device" in your datanode? what is it exactly?
By default Spark uses the /tmp directory to store intermediate data. If you actually do have space left on some device --
you can alter this by creating the file SPARK_HOME/conf/spark-defaults.conf and adding the line.
Here SPARK_HOME is wherever you root directory for the spark install is.
spark.local.dir                     SOME/DIR/WHERE/YOU/HAVE/SPACE
